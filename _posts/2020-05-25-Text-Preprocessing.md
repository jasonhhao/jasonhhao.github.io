---
layout: post
title:  文本预处理
date:   2020— 00:00:00 +0800
categories: 自然语言处理
tag: 文本预处理
---

* content
{:toc}


<h2 align="center">宏观理解</h2>

自然语言处理无疑也就是对人类自然语言进行一些理解，这些自然语言的形式包括语音和文字。所以说NLP = NLU(自然语言理解) + NLG(自然语言生成)，NLU是从语音或者文本去提取语义，NLG是从语义之中去
提取文本或者语音。自然语言最大的难点在于同一意义下的多种表述，还有很多的一词多义，比如我爱吃苹果和我爱用苹果，这里的苹果一个是水果一个是品牌。还有语言的多模态，
比如在不同情境下的相同语言含义不一样。当我们拿到一坨的文本信息之后，到建模之间需要怎么样的处理？通常来讲自然语言处理的pipeline为：

先搜集到所需的文本 ===> 对文本进行中文/外文分词 ===> 数据清洗，去掉一些无用的符号和停用词 ===> 对剩下的分词进行标准化(词干提取/词性还原)，比如把英文单词所有的进行时过去时的词全部打回原型 ===> 提取文本特征，
比如使用w2v/神经网络生成word embedding ===> 剩下就可以对下游任务进行建模训练

我们把建模之前的所有过程叫做文本预处理。

<h2 align="center">微观分析</h2>

针对不同的任务也有不同的文本预处理方案，对中文和英文的处理流程和途径也都是有区别的。下面简单列举一些，随时增加。

<h3>分词</h3>

市面上有很多的分词包可以直接使用，无需我们自己实现，比如中文分词用的多的jieba。
那么他们这些分词库是怎么进行的分词呢？下面来简单介绍几种中文分词方法。

1. 最大匹配分词算法(forward-max matching)：
其主要原理都是切分出单字串，然后和词库进行比对，如果是一个词就记录下来， 否则通过增加或者减少一个单字，继续比较，一直还剩下一个单字则终止。其中最大匹配
也分为前向最大匹配和后向最大匹配，其中统计下来更优的方法是后向最大匹配。具体例子在[百度百科](https://baike.baidu.com/item/%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/10967757?fr=aladdin)里很好理解这里就不赘述了。
但缺点是最大匹配往往无法提取到语义信息，也就是无法知道上下文对这么词的影响。

2. 语言模型分词：
语言模型就是一个模型可以学出来每个词出现的概率，比如“我今天__了米饭“，通过语言模型的上下文关系我们可以知道“吃”放在这里的概率是最大的。所以语言模型的意义在于
告诉机器或者人此时此刻你选择哪个词是最靠谱的。所以它就可以做到利用上下文进行分词。
语言模型是怎么考虑到上下文的呢？比如我们有一句话[你，今天，吃，米饭，了，吗]，所谓的n-gram模型就是当判定米饭这个词的时候，考虑到了在米饭之前的所有词，写作
![](https://latex.codecogs.com/gif.latex?P%28s%29%20%3D%20P%28%5Bw_1%2C%20w_2%2C%20...%20%2C%20w_n%5D%29%20%3D%20P%28w_1%29P%28w_2%7Cw_1%29...P%28w_k%7Cw_1%2Cw_2%2C...%2Cw_%7Bk-1%7D%29)
但是我们发现越往后的词会跟前面所有的词都有关，这样计算越来越困难，结果也变得越稀疏，所以我们干脆固定下来当前的词只和前面多少个词有关，比如uni-gram就是只和它本身有关，bi-gram就是只和前面1个词有关。
所以我们最后得到了整个句子的概率，这个概率越高，说明这句话越靠谱越像一句人话。


<h3>拼写纠错</h3>

大家都用过word文档吧，当你输错一个单词的时候它会画一条红线警示你你可能拼错了。那么在拿到文本的时候，我们不能肯定里面没有错误拼写，如果直接拿来用肯定会影响
语义提取的准确率，所以在预处理的时候我们也可以进行一些错误单词的纠正。那么我们怎么找到错误呢？我们可以拿每个词在词表中进行对比，如果词表中没有这个词，可以姑且
判定这个词是错的。那怎么生成候选词呢？
1. 对于词表中的每个词，都跟这个词计算出来一个最小编辑距离。然后选出最小的n个词返回给用户。但是这种方式太慢了，怎么改进？
2. 我们可以直接生成编辑距离为1，2的全部词作为备选，然后过滤这些词。怎么过滤呢？我们可以计算P(某个备选词|用户输入的词)，然后选择最大的概率。根据贝叶斯公式，
我们可以写成P(某个备选词|用户输入的词) = P(用户输入的词|某个备选词)P(某个备选词) / P(用户输入的词)也就正比于P(用户输入的词|某个备选词)*P(某个备选词)。至于P(用户输入的词|某个备选词)
怎么计算呢？可以根据用户的log信息提取出来。这个模型称为**noisy channel model**。

<h3>停用词过滤</h3>

网上有很多的停用词词表，在我们拿到文本之后可以查词表把在词表的词去掉就好。但是也不是说任何任务都需要去掉停用词，如果我们在做情感分析的任务，那么很多的
定冠词the，an，they，is就意义不大了。可是如果我们需要训练机器翻译模型，那就不必去掉停用词，否则会影响语义的理解。

<h3>词标准化</h3>

尤其是英文的文本处理时，需要进行标准化，可以理解为我们要把每个词还原成它最基本的形式，比如把playing还原成play，比如把“the US”和“U.S.A”统一写成USA。
那么一般怎么去标准化呢？一般有两个途径，至于怎么选择，这个就跟具体下游任务有关了：
- Stemming词干提取：抽取词的词干或词根形式（不一定能够表达完整语义）
- Lemmatization词形还原：把一个任何形式的语言词汇还原为一般形式（能表达完整语义）















