---
layout: post
title:  自然语言处理入门
date:   2020-07-19 00:00:00 +0800
categories: 自然语言处理
tag: 自然语言处理入门
---

* content
{:toc}


## 课程导学

同学好，我是Jason老师。今天我们这节课的主题是自然语言处理，在学习自然语言处理之前，如果同学不了解深度学习老师建议
先报名《深度学习入门》。我们这节课虽然不会涉及很深，但是我们需要用到深度学习模型来完成我们今天的项目。如果同学准备好了
我们就开始吧～

自然语言处理（缩写NLP）如今也是发展的火热。但是因为业内公认它是AI领域里比较难的子领域，所以很多技术还不成熟，很难追平人类的准确率。例如经常被人们吐槽的“有道翻译”或“百度翻译”。他们也只能很委屈的说声：“臣妾真的做不到啊～！”。所以老师认为自然语言处理是一块挖掘不久的宝矿，只要同学有入行的想法就赶紧行动起来，一切都才刚刚开始！

我们本节课会通过一个自然语言处理的典型任务【文本分类】来让同学对自然语言处理有一个全面的感观。我们会用几千份“某团”外卖的真实用户评价来让计算机识别出这个评价是好评还是差评。

在下面的课程中，老师会在第一章先介绍一下自然语言处理的概念和背景，在随后章节的实战项目中我们还会学到：

1. 如何对文本进行预处理
2. 如何转换词向量
3. 如何使用深度学习模型进行分类

激动不，让我们开始吧！

## 第一关

#### 1.1 概念

我们从小也是被“自然语言处理”过的，这个处理的过程就是上！语！文！课！

语文课上老师会教我们看图说话；会教给我们阅读理解；会教给我们通过给定题目写出一篇作文。
后来我们长大了，还需要学习说话的艺术；如何从一天的会议中精准提取老板布置的任务；如何有针对性回答客户的疑问等等。

我们这一生都需要通过自然语言进行理解、思考和表达。所谓自然语言，呈现形式无非就是文字和语音嘛。所谓处理，也就是思考的过程，我们会在脑子里想一想为啥我妈会突然喊我全名了？！

现在我们的对象不是人类而是计算机，我们仍然要教给计算机如何看图说话、阅读理解、写一篇高考作文，甚至前段时间出现的作诗机器人。

我们知道在人类的交谈中，总是往复循环着两个过程，一个过程是听到并去理解对方说的话，另一个过程是清晰地表达自己的想法给对方。所以在广义上，我们会把【自然语言处理】分成两个类别，一个叫【自然语言理解】，
来让机器理解人类在跟它说什么；另一部分叫【自然语言生成】，让机器懂得如何告诉人类它自己的想法。

下面我们来看看现实生活中都有哪些【自然语言理解】和【自然语言生成】的应用呢？

#### 1.2 日常生活应用


【自然语言理解】我们说过是希望机器可以具备正常人的语言理解能力。例如我们邮箱里都具备的垃圾邮件过滤功能：

<p align="center"> 
  <img src="/imgs/pics/1.png">
</p>

还有像我们今天的分类项目，通过对用户评论的理解分出好评和差评，就和给定一些邮件分成是垃圾邮件和不是垃圾邮件一个道理。

【自然语言生成】我们也说过就是希望机器可以将自己的想法转化成人类能看懂的语言。例如作诗机器人：

<p align="center"> 
  <img src="/imgs/pics/2.png">
</p>

在现实生活中更多的还是【自然语言理解】和【自然语言生成】所结合的应用，例如各大网站上都渐渐出现了的机器人客服：

<p align="center"> 
  <img src="/imgs/pics/3.png">
</p>

还有早就被网友玩坏了的Siri:

<p align="center"> 
  <img src="/imgs/pics/4.png">
</p>


我们认识了自然语言处理广义上的两大类，但是我们知道，一栋房子的搭建不只是由一个团队的人完成的，有设计团队有采购团队还有建筑工人。
我们这两大类别的每个也都是由很多子领域的人一起努力创造出来的。我们作为一个NLPer，还要对自然语言处理的任务分类有所了解，也可以帮助我们寻找一个感兴趣的岗位。

#### 1.3 任务细分

这个分类其实业内不统一，分类标准也是五花八门，所以我们只需要宏观来感受一下一个应用的背后都会有哪些团队的人在努力着。
我们从小到大来说，也就是从词到句，最后到整个文本。

1. 词级别的任务
  1. 分词：把“我爱自然语言处理”分成“我/爱/自然/语言/处理”
  2. 词性标注：把分词后的每个词标注上词性
  3. 命名实体识别：识别出一句话中的实体，例如“小明得了流感”中会识别出“小明”和“流感”
  4. ...


2. 句级别的任务
  1. 下一句话预测：给出“我今天请假了”，预测下一句可能是“因为我得了流感”
  2. 问答系统
  3. 机器翻译
  4. ...


3. 文本级别的任务
  1. 文本分类：这本书是属于惊悚呢还是都市爱情呢？
  2. 文本摘要：给定一篇文章，生成简要总结
  3. 文本相似度：两篇文章的主题是不是类似？
  4. ...

所以一个简简单单的写诗机器人，也许就会用到分词、词性标注、分类、下句话预测等等。但是同学不用怕，那些研究怎么分词怎么标注都是科学家语言学家做的事情，如果我们只是想做一个项目，世面上已经有了很多库可以供我们直接使用～

好了，我们对于基本的自然语言处理认识的差不多了，接下来我们可以开始研究一下实战项目了，老师会先简单介绍一下我们需要的基础知识，再同时边写代码边讲解。



## 闯关练习

1. 下列描述错误的是：

A. 机器人客服用到了【自然语言理解】和【自然语言生成】
B. 垃圾邮件识别用到了【自然语言理解】
C. 机器翻译只用到了【自然语言生成】

答案：C， 机器翻译和人类翻译一样，都需要先理解一门语言，再经过处理生成另一门语言，所以都会用到。



## 第二关

#### 2.1 读取并观察数据

首先我们先打开我们的数据，同学只要运行下面两行代码就可以拿到数据并且保存在data中。

```
import pandas as pd

csv_url = "https://rb.gy/ouyn41"
data = pd.read_csv(csv_url)
```

我们先看一下数据里的好评差评数量：

```
data['label'].value_counts()

output：
0    7987
1    4000
```

可见这份数据共有12000条评论左右，其中好评有4000条，差评约8000条。我们为了让数据更加均衡，保留所有好评的同时从差评中随机抽取4000条出来：

```
pos = data.loc[data['label'] == 1]  # 拿到所有标签为1，也就是好评的评论
neg = data.loc[data['label'] == 0]  # 拿到所有标签为0，也就是差评的评论
neg = neg.sample(4000)              # 从差评中随机抽取4000个
train = pd.concat([pos, neg])       # 把4000好评和4000差评拼接在一起
train = train.sample(frac=1)        # 随机打乱8000份评论
train.head()
```

我们可以看到我们的数据是长这个样子的：

<p align="center"> 
  <img src="/imgs/pics/5.png">
</p>

标签为1就是好评，标签为0就是差评，后面是对应的评论。

现在我们把label和review分开，方便后续的处理：

```
label = train.label
review = train.review
```

好，接下来我们看看这些数据里有没有空值，也就是null：

```
review.isnull().any()

output：
False
```

OK，没有空值。但是很显然我们不能直接把这一堆文字给到任何模型，那么接下来我们就要对这些评论进行一个预处理。


#### 2.2 预处理流程概述

如果同学有机器学习基础，我们知道特征是一些数据点，也许是离散的也许是连续的。或者同学有计算机视觉的基础，我们知道一个图片的特征就是他的0-255像素点分布，也是由数字组成的。

那么现在如果我们要把一句话当作特征，同学想想如何去表示它？也用一串数字？那怎么得到这串数字呢？我们当然是希望这串数字可以很好的代表这个句子。

对的！这个问题就是所有NLP问题的基础，也是导致NLP很难的原因，至今都没有足够成熟且易用的方法。

那既然我们不能轻易找到一个很好的序列来表示句子，我们退而求其次，表示词组行不行？

行！但是问题来了，我们要想找词组的表示序列，第一步我们就要知道这句话中有哪些词组。对于英文来说简直不要太容易，只要有空格的地方分开就完成了。那么对于中文来说怎么办？

**这是我们面临的第一关：分词。**

就算我们有办法分词了，例如把一个句子分成了[我，今天，吃，了，苹果]。那我们在想怎么表示每个词之前，先想到我们要不要每个词都表示出来啊？像句子中这种“了”我们难道也要表示吗？如果“了”都要表示，那我们需要表示标点符号吗？

**这是我们面临的第二关：去停用词和符号。**

这个貌似不难实现，无非就是遍历一遍所有单词，如果这个单词我们不想要就给扔掉。那我们有了扔掉停用词后的单词列表[今天，吃，苹果]后，如何生成他们的表示序列呢？

**这是我们面临的第三关：词表示。**

所以我们总结一下，在拿到一份数据之后，我们可以先做一个分词，再从分词中扔掉那些没有什么意义的符号，这个过程叫做中文文本预处理。预处理后，把剩下的词用一些很神奇的办法转换成序列：

<p align="center"> 
  <img src="/imgs/pics/6.png">
</p>

接下来还等什么，我们开始预处理吧

#### 2.3 分词

中文的分词我们已经有了专业的库可以直接使用，叫做jieba分词，同学可以直接pip install jieba进行安装。
随后我们使用的时候就可以把一句话传给jieba.lcut(一句话)：

```
import jieba

review_tokens = review.apply(lambda x: jieba.lcut(x))
```

现在我们已经把review里的8000条数据都分好了词，来看一下：

<p align="center"> 
  <img src="/imgs/pics/7.png">
</p>

如果我们的文本是英文的话，可以使用nltk中的tokenizer。也是像jieba一样pip安装一下就可以直接使用了。


#### 2.3 去停用词

在这个环节，我们在去掉停用词的同时，也顺便把一些符号和英文去掉。网上有很多中文的停用词表，同学可以下载下来一份，我们这次就用百度总结的停用词表，可以点击[这里](https://github.com/goto456/stopwords)下载。

第一步就是我们把词表里的词都拿出来放到一个list中：

```
with open('baidu_stopwords.txt') as f:
    lines = f.readlines()

stop_words = [word.strip() for word in lines]
```

随后我们定义一个函数，让传进来的每一句话都去掉停用词、数字、字母和一些奇怪的符号：


```
import re

def textFilter(wordList):

    # 去掉停用词
    wordList = [word for word in wordList if word not in stop_words]

    # 去掉字母数字和标点符号
    r1 = "[a-zA-Z0-9\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、：；;πㅎωฅづ《》“”~ᐛ@#￥%……&*（）]+"
    textString = re.sub(r1, '', ''.join(wordList))
    wordList = jieba.lcut(textString)

    return wordList
```

然后我们把review中的每一句话都apply这个函数：

```
review_tokens = review_tokens.apply(textFilter)
```

最后我们看一下结果，发现这些句子干净清爽了许多：

<p align="center"> 
  <img src="/imgs/pics/8.png">
</p>

到目前为止我们预处理就完成了，接下来的任务就是把我们留到最后的所有词找一个表示序列来代替。


## 闯关练习

1. 下列步骤中正确的是：

A. 先去除掉停用词，随后分词
B. 分完词后直接就可以生成词向量
C. 要先分词，然后再去除停用词

正确答案：C
解析：我们的一般步骤为【分词 -> 去除停用词 -> 生成词向量】



## 第三章

#### 3.1 独热编码

在找到一个词的表示之前，我们已经有的是一大批文本，我们叫这些文本的集合为【语料corpus】。我们可以语料中包含的所有不同拼写的词找出来组成另一个集合，叫做【词库】：

例如我们的语料为[[今天天气真好],[明天天气不好],[后天天气也不好]]，那么我们最终的词库就是[今天，天气，真好，明天，不好，后天，也]。

我们有了词库后，独热编码会首先进入人们的视线，对机器学习熟悉的同学肯定对独热编码不陌生了。这时我们可以把一句话中在词库中出现的位置标为1，其余为0。那么：

> 我们的词库为[ 今天，天气，真好，明天，不好，后天，也 ] <br>
> 编码前：[ 明天天气不好 ] <br>
> 编码后：[ 0，1，0，1，1，0，0 ]


好像也可以，我们确实可以得到一个表示序列。但是有个问题是这样的编码体现不了单词的频率。比如[ 明天天气不好 ]的编码和[ 明天天气不好不好 ] 是一样的。那么如果一个词在文本中出现了很多次也许代表它很重要，很能体现文本的中心含义，那么怎么把频率也加进去呢？

#### 3.2 词袋模型

之前我们是如果在词表中出现了这个词，就标记为1。那么我们可不可以标记成它出现的次数呢？

例如：

> 我们的词库为[ 今天，天气，真好，明天，不好，后天，也 ] <br>
> 编码前：[ 明天天气不好 ]      编码后：[ 0，1，0，1，1，0，0 ] <br>
> 编码前：[ 明天天气不好不好 ]   编码后：[ 0，1，0，1，2，0，0 ]

这样在词表中“不好”的位置就可以标记为2。这种计算方法我们叫做【词袋模型bag of words】。

这时候又有人跳出来反对说：“那万一说话的人口吃咋办啊？”

诶也对，如果一个句子是这样的 [我明天想去..去..去..去..公园！]。有一说一，这时候“公园”不比“去”更重要吗！

随后人们就想那要不在有频率的基础上，再标记一下它的重要程度？

#### 3.3 TF-IDF

那么什么样的词才重要呢？我们当然还是认为在一篇文章里重复越多的词越重要，但是如果这个词不仅仅在这篇文章中重复多次，而是在所有文章中都有重复，那它的重要程度肯定要降低了。既然有了这个思路，我们就可以使用一个算式来计算出每个词的重要程度，这个算式就是大名鼎鼎的TF-IDF。

> TF（term frequency）来表示在当前文章中这个词出现的次数<br>
> IDF（inverse document frequency）来表示这个词在多少篇文章中出现过

最后我们把这两个结果相乘就可以得出当前单词的表示。

但是这时候又有人跳出来说：“你这样虽然表示了一个词，可是跟上下文好像没有关系啊！”

确实，我们之前都是把单个词拿出来想办法怎么表示它，这样就失去了上下文的含义。我们当然希望西瓜和香蕉的词表示是相近的，西瓜和望远镜的词表示是不挨着的。

说到处理全局信息，同学会不会想到神经网络？不错，后来人们开始摒弃之前的统计表示法，尝试使用神经网络来编码一个单词。

#### 3.4 word2vec

相信想学习NLP的同学一定听过word2vec这个模型，它算是一个文本表示的里程碑了。但是很多同学都以为它是一个模型，错！就像我们做饭的酱油一样，它分成生抽和老抽，word2vec它也是两个模型的统称，一个叫skip-gram模型，另一个叫cbow模型。当我们说使用word2vec的时候，我们可以从这两个模型中选择一个使用。

他们的中心思想其实很简单，就是如果我们要考虑上下文，那我们就干脆用上下文去预测中心词，或者我们也可以用中心词去预测上下文词。

例如[ 明天天气不好 ]这句话，我们可以用[明天]和[不好]来预测[天气]，或者反过来用[天气]来预测[明天]和[不好]。这样我们通过神经网络的训练不就可以把上下文的信息都融合进去了吗？

对的！这就是word2vec的两种模型的思想：

> skip-gram模型：用中心词去预测上下文 <br>
> cbow模型：用上下文去预测中心词

这些模型强大到什么程度呢？在它训练完的词表示中:

> “国王”的词表示 - “男人”的词表示 = “王后”的词表示 - “女人”的词表示

如果我们可视化出来所有的单词表示，我们可以看到所有相近含义的词都被聚集在很近的地方：

<p align="center"> 
  <img src="/imgs/pics/9.png">
</p>


所以它可以充分的满足我们要求联系上下文的要求！在我们的项目中，当然也要使用它！

我们可以直接使用gensim里的库调用word2vec：

```
import gensim

w2v_model = gensim.models.Word2Vec(review_tokens, size=50, window=5, min_count=1)
```

接下来我们可以任意给出一个词来通过w2v_model得到一个词表示，例如：

<p align="center"> 
  <img src="/imgs/pics/10.png">
</p>

我们可以得到一个50维的向量，接下来我们要把我们的语料中的所有句子中的每个单词都替换成这样的词表示：

```
review_emb = [[w2v_model.wv[word] for word in review] for review in review_tokens]
```

好。完成！但是还差一步，就是现在我们的句子长度不统一，有的句子就一两个字，有的甚至几百个字。在后面的模型训练中，模型需要统一一下。

我们先可视化一下每个句子的长度：

```
import matplotlib.pyplot as plt

plt.hist([len(review) for review in review_tokens], bins = 30)
plt.show()
```

<p align="center"> 
  <img src="/imgs/pics/11.png">
</p>

可以看到，大部分的评论都在0-20个词之间，超过30个词的就很少了。所以我们可以统一所有的句子长度为30，那么超过30个词的句子我们就要截断，没超过30的句子我们可以后面补上0。

```
from keras.preprocessing.sequence import pad_sequences

review_emb=pad_sequences(review_emb,maxlen=30)

```

好，经过pad_sequences之后，多退少补。我们可以检查一下现在review_emb的shape是啥样的：

```
review_emb.shape

output:
(8000, 30, 50)
```

(8000, 30, 50)的第一维表示我们有8000个评论，第二维表示我们每个评论有30个单词，第三维表示我们的每个单词有长为50的词表示。

好了完美！接下来我们就可以训练模型了！


## 闯关练习

1. 下列关于TF-IDF说法错误的是：

A. TF（term frequency）表示在当前文章中单词出现的次数
B. TF-IDF的结果为TF和IDF的结果相加
C. IDF（inverse document frequency）表示这个词在多少篇文章中出现过

正确答案：B
解析：TF-IDF的结果为TF和IDF的结果相乘，并非相加

2. 下列关于词向量模型说法错误的是：

A. TF-IDF模型可以考虑到上下文信息
B. word2vec模型是skip-gram和cbow的总称
C. word2vec模型可以考虑到上下文信息

正确答案：A
解析：TF-IDF是一种概率模型，主要解决了单词重要性的问题，但是依然无法解决上下文相关信息


## 第四章

#### 4.1 训练模型

首先我们把数据分成训练集和验证集，这个直接调用train_test_split就好：

```
from sklearn.model_selection import train_test_split

label = label.to_numpy()
review_emb = review_emb.astype('float32')
x_train, x_test, y_train, y_test = train_test_split(review_emb, label, test_size = 0.2)
```

现在我们可以开始建模了，说到模型的选择上这又是一个很大的话题了。虽然Transformer系列或者说Bert系列现在是最新的效果最好的，但是他的参数量也是巨大的，随着计算机硬件的进步，很多模型都可以靠着强大的算力和内存展现优异的成绩。如果想要训练一个Bert可能一个CPU就不好完成了，所以很多公司或者个人在经济压力的迫使下。。会选择一些效果不差也不耗费大量财力的模型替代。

我们接下来要使用的LSTM就是其中之一，他是循环神经网络的一种，算是普通RNN的一种优化，如今仍然活跃在各种任务中。我们使用起来也很简便，tensorflow或者pytorch都已经为我们封装好了。

接下来我们使用tensorflow的Sequential容器来自己搭建一个LSTM分类器。

```
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential()
model.add(layers.LSTM(256))
model.add(layers.Dense(1))
model.add(layers.Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

我们首先往model里面添加一个LSTM层，括号里的参数同学不用太纠结这不是本节课的重点，只要知道这个数字越大的话模型越复杂就好。
因为我们的任务是二分类，所以在输出结果时我们只要一个sigmoid激活函数来帮我们判断是0还是1就好。所以我们在LSTM之后接上一个全连接层layers.Dense(1)。这里的参数1指的是我们只输出一个数字，0还是1即可。

最后我们指定一下损失函数binary_crossentropy、优化手段adam和评估指标为准确率accuracy。

到此为止我们的模型就搭建完成了，接下来就是让它训练起来！

```
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30, batch_size=64)
```

我们传入（x_train, y_train）作为训练集，指定（x_test, y_test）为验证集。epochs是用来指定我们要训练的轮数，这个同学可以自己决定。最后batch_size是指定我们从数据里同时拿出64条评论放入模型训练。

这个过程的时间长短就取决于同学设置的LST复杂程度和epochs轮数了...

A few moments later...

Ok训练完成，老师的模型从一开始的73%准确率训练到了78%左右。同学可以用x_test, y_test来评估一下自己的模型：

```
scores = model.evaluate(x_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))
```

虽然老师只有78%准确率=。= 但是也足够去判断一些比较容易的句子了。
接下来我们随便编几句话来看一看模型的预测结果，比如我们输入【他们态度很差，分量也很小】。

但是不要忘了我们还要把这个句子做一下预处理，就和之前所有的句子一样，最终把他的词向量，也就是词表示喂给模型。我们正好通过这个过程再来回忆一下整个流程：

```
test = "他们态度很差，分量也很小"
# 第一步：分词 ==> ['他们', '态度', '很差', '，', '分量', '也', '很小']
test_tokens = jieba.lcut(test)

# 第二步：去除停用词和标点符号 ==> ['态度', '很差', '分量', '很小']
test_tokens = textFilter(test_tokens)

# 第三步：用word2vec得到每个词的词向量，调整好输入的维度
test_emb = [w2v_model.wv[word] for word in test_tokens]
test_emb = np.expand_dims(test_emb, axis=0)

# 第四步：喂给模型预测，打印结果
result = model.predict(test_emb)
print('好评' if result[0][0] > .5 else '差评')
```

<p align="center"> 
  <img src="/imgs/pics/12.png">
</p>

再换一句夸赞的比如【量大，味道也不错，性价比超高。】：

<p align="center"> 
  <img src="/imgs/pics/13.png">
</p>


我们的模型很基础很简单，如果同学想提升准确率的话不妨多加几个LSTM层试一试～


## 第五章 总结

本节课我们通过训练一个循环神经网络的模型来预测某团外卖评价的好坏，学习到了如何预处理文本，都有哪些方法可以把文字转换成词向量。

<p align="center"> 
  <img src="/imgs/pics/14.png">
</p>

我们的预处理包括了使用jieba对中文分词，然后去除掉一些停用词和乱七八糟的符号。然后把预处理过的词通过word2vec转换成词向量。除了word2vec我们还有古老的词袋模型和tfidf，当然同学感兴趣可以尝试用最新的Bert来转换词向量。最后我们用了一层的LSTM模型来做二分类。

细心的同学注意到了老师在上图中预处理的部分还标记有一串省略号，这是留给同学的课后作业，我们除了分词和去停用词以外当然还有别的预处理步骤，那么还有什么呢？

（老师小声提醒一下：例如我们在文本中发现了错别字怎么办呢？拼写错的单词呢？）

相信你已经get到了！

下课！同学再见～
