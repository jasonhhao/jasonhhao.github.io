---
layout: post
title:  深度学习入门
date:   2020-06-10 00:00:00 +0800
categories: 传智播客
tag: 传智播客
---

* content
{:toc}


## 课程导学

Hello 同学你好，我是杰森老师。欢迎你今天可以跟我一起来探索深度学习的奥秘。我相信打开这门课的你一定听说过人工智能这个词汇，没听过也不要紧～但是你一定知道一些人工智能在现实中的应用。不信？我来举几个你一定知道的例子：

来，打开手机，你的照相机有没有自动美颜？对的，这些所谓的美颜，还有前段时间火过一时的看看自己老年相貌小应用，这些样貌的生成其实都是人工智能的杰作。

<p align="center"> 
  <img src="/imgs/czbkpics/1.jpg">
</p>

再来举一个例子。百度翻译、有道翻译、谷歌翻译你肯定用过吧，这可不是程序猿一句一句先录入答案的然后再通过你给的句子找到对应的答案哦。它们的背后都是一个人工智能模型在接收我们的输入，然后通过一些神奇的算法再把它觉得最有可能的结果告诉我们。

<p align="center"> 
  <img src="/imgs/czbkpics/2.png">
</p>


你看，其实人工智能已经默默地潜入到我们的生活了，它可以更好的帮助我们完成我们的目的，让我们的生活变得更加轻松自如。

如今越来越多的岗位逐渐被人工智能取代，但是不用怕，人工智能也为我们创造了更多的岗位，不仅不用害怕找不到工作，而且“钱途”很大有木有～

<p align="center"> 
  <img src="/imgs/czbkpics/3.png">
</p>

只要你跟着老师上完本次课程，你将会入门人工智能并且可以自己构建一个人工智能模型--教给电脑认数字！

在构建模型前，我们首先看看我们都需要哪些前置知识：
1. 了解深度学习的概念及应用
2. 了解神经网络是个什么东西
3. 了解都有哪些深度学习的模型
4. 了解有哪些工具可以帮助我们构建深度学习模型
5. It's ready to 开发自己的深度学习模型！！！

## 第一关 石猴出世

#### 1.1 深度学习的概念

日常生活中我们听到最多的词就是人工智能，但是要注意人工智能并不等于深度学习哦。今日头条为我们绘制了一个图谱。通常来说人工智能包括机器学习、深度学习。它们之间有一定的交集但是我们一定要区分开它们。

<p align="center"> 
  <img src="/imgs/czbkpics/5.jpg">
</p>

我们所说的深度学习是指由**多层神经网络**所架构的模型。我们在完成一个任务的时候肯定要有的就是输入和输出，比如机器翻译中我们输入中文，输出英文。在神经网络中，我们把它们叫做输入层和输出层。从输入层到输出层之间还会有很多层用来计算，它们负责如何把输入转化为输出。这些中间层我们称为“隐含层”。如果一个神经网络含有**多个**（大于1个）隐含层，我们就管这个网络叫深度神经网络，学习这个模型的过程也就叫做深度学习。因为它在各项任务中表现突出，在越复杂的任务中往往比普通机器学习模型强悍越多，所以日益发展壮大，独成一派。

#### 1.2 深度学习的发展史

我们知道深度学习模型是由深度神经网络架构而成的，神经网络诞生于1943年，但是那个时候因为计算机硬件的落后，更多的概念仅仅停留在学术层面。随着计算机处理器的计算速度和存储越来越强大，在1957-1958年，终于把单层神经网络用在了人工智能应用（模式识别）上，实现了神经网络的第一次兴起。在1986年迎来了两层神经网络，实现了第二次兴起。但是后来又因为CPU算力的不足，人们把精力都放在了普通机器学习模型上，比如KNN，梯度提升树等。终于到了2012年卷积神经网络的诞生，在ImageNet竞赛中秒杀所有普通机器学习模型，迎来了多层神经网络的时代并开启第三次兴起。

<p align="center"> 
  <img src="/imgs/czbkpics/4.png">
</p>

很荣幸，同学，我们现在就处在神经网络第三次兴起的兴盛阶段！随处可见的人工智能应用已经彻底的潜入我们的生活。

#### 1.3 深度学习随处可见

其实我们身边的很多人工智能应用都属于深度学习的范畴。例如：在商业中比如人脸识别，指纹解锁，虹膜认证，自动驾驶，还有对视频内容的提取。在医学中比如肿瘤的识别，健康状况的监控，手术机器人等。这些深度学习模型都跟图像有关，我们管涉及图像的深度学习领域叫做计算机视觉。

<p align="center"> 
  <img src="/imgs/czbkpics/6.jpeg">
</p>

同样的，在商业中我们还有机器翻译、情感分析、文章摘要提取等；在医学中，我们还可以通过患者对病情和病史的叙述，提取出关键信息，辅助医生为病情作判断等等。这些深度学习模型都和语言文字有关，我们管涉及语言的深度学习领域叫做自然语言处理。

所以说我们的深度学习可以处理两种任务，关于图像的任何和关于语言文字的任务。此刻相信你已经心里有谱了，老师邀请你跟随我一起走进深度学习的世界，并且我们要仅用7行代码实现自己的深度学习模型！



## 闯关练习

1. 下列选项中属于深度学习的模型是：

> A. 单隐含层神经网络
> B. 梯度上升树
> C. KNN
> D. 多隐含层神经网络

正确答案：D，我们管由多个隐含层组成的神经网络叫做深度学习模型。

2. 下列描述中正确的是：

> A. 人工智能包括深度学习
> B. 机器学习包括深度学习
> C. 神经网络包括深度学习
> D. 机器学习不包括神经网络

正确答案：C， 由AI知识图谱我们可以看到，深度学习是完全属于神经网络领域内的。



## 第二关 刨根问底


#### 2.1 神经元

我相信你之前一定听说过**神经元**这个词汇，我们的生物老师告诉我们动物体内的神经元可以传递信号。那你知道计算机世界中的神经元吗？

1943年，神经学家沃伦和逻辑学家沃尔特，发现既然计算机不如我们的大脑，那我们能不能让计算机也模仿大脑进行工作，最起码也要跟我们的大脑一样强嘛！所以他们把动物大脑中的神经网络原封不动地搬到了数学模型中。所以在他们的模型里面，一个简单的人工节点，我们就称为神经元。像动物体内的神经元一样，它也可以帮助我们对输入的信息进行整合并且输出给下一个神经元。

就好比我们的眼睛看到的物体，通过一连串的神经元排排坐，一个转告另一个：
神经元A对神经元B说：“喂，老师看到了一个物体，它头发很长，大大的眼睛，还咧着嘴。” 神经元B整合信息之后对神经元C说：“喂，老师看到了一个大眼睛的女生，好像还在笑。” 神经元C再整合信息对神经元D说：“喂，老师看到一个爱笑的大眼睛女生！” 最终传递到大脑，大脑说：“行了我知道了，哎他又看到美女了。”

此时老师拍拍旁边的哥们：“诶你看，美女诶！”

我们管眼睛看到了物体形状叫做**输入**，这个输入可能会很零散，零散到像一堆乐高拼图碎片你根本看不出来它是什么，随着神经元的辛勤整合和传递，我们才能在大脑中产生输出，来判断眼睛看到的是个什么～

再后来1958年Frank Rosenblatt觉得这个思想完全可以创造一个可以识别物体的机器啊，就创建了一套算法，后来人们把这套算法叫做“感知机”，也就是大名鼎鼎的**人工神经网络**。

从此人工神经网络这门学科也被人们称为“仿生学”。

此刻在你大脑里的神经网络应该是这样的：

<p align="center"> 
  <img src="/imgs/czbkpics/7.png">
</p>

但是问题来了，我们现在只是把输入给到了一个神经元A，可是我们的眼睛看到一个物体是由很多个神经元共同接收信息的呀？别着急，我们完全可以多复制几份呀。

#### 2.2 单层神经网络

我们复制几份后，眼睛看到的物体就由多个神经元共同接收信息了～这就是单层神经网络。
顾名思义，既然是单层，那就是只有一层。如果回到我们上一个老师看美女的例子中，就会变成这样：

<p align="center"> 
  <img src="/imgs/czbkpics/8.png">
</p>

我们照样还是有一个输入，有一个大脑作为输出，在他们之间我们有一列的神经元排排坐共同接收视觉信息，和上一个例子的区别是这回它们不能沟通！A1不能把自己看到的东西偷偷告诉A2！它们只能把自己看到的东西直接告诉大脑！现在的情况就变成了：

神经元A1发现老师正在看到的物体有长长的头发，便对大脑说：“老师看到了长长的头发！”
神经元A2发现老师正在看到的物体有大大的眼睛，便对大脑说：“老师看到了大大的眼睛！”
神经元A3发现老师正在看到的物体正在咧着嘴，便对大脑说：“老师看到了物体咧着嘴！”
大脑接收到了神经元们的通知说：“行了我知道了，哎他又看到美女了。”

此时老师又拍拍旁边的哥们：“诶你看，又一个美女诶！”

这种情况下，我们管中间的那一列神经元们叫做一层隐含层，所以由输入（眼睛）->一层隐含层->输出（大脑）这样的组合我们就叫做单层神经网络，也叫单层感知机。

是不是很简单，但是别看它的结构很简单，它的能力是很强的。在数学中它还有一套定理叫做“万能近似定理”，指的是如果你手头上有一个包含一层隐含层的网络结构，只要你这一层有足够多的神经元排排坐（例如有50个/100个/...），那么你这个神经网络就可以做到任何的事情！

厉害吧，可是如果你对神经网络有所了解或者你偷偷看到了后面的内容，就不禁会有一个疑问：既然单层神经网络已经这么厉害了，我们为什么还要有多层神经网络呢？

为了解释这个疑问，我们要引入神经网络“参数”这个概念。打个比方，老师在上次看到美女的超市买菜，走到草莓摊位前问老板：“老板你这草莓咋卖啊？” 老板说：“10块钱1公斤，20块钱2公斤。30块钱3公斤。” “害，老板你直接说每公斤10块钱不就好了嘛！”

我们把这个场景用神经网络代替一下，输入是10块钱、20块钱、30块钱，输出是1公斤、2公斤和3公斤，那么中间的神经元怎么去转化这个呢？没错就是把输入除以单价10就好了，当我们发现了这个规律之后，无论你告诉我你要买多少钱的草莓我都能算出来你买的多少公斤。神经网络的参数，就是那个单价10。

神经网络做的事情，就是把我们人类不能在脑子里计算出来的参数，帮助我们计算出来。

好了，回到之前的问题。我们知道了神经网络就是在学习所谓的参数，我发誓你用单层一定可以学习到最好的参数，但是我又没说你肯定能学到。换句话说，我保证你有一生赚够一千万的能力，但是我又没说你一定能赚到。。

一万匹草泥马呼啸而过。OK吧，那既然我一个人未必能达到，我找几个小伙伴组团就简单很多了吧。于是单层神经网络就变成了多层神经网络。

#### 2.3 多层神经网络

我们此时选中第一层神经元A1-A3，右键复制粘贴重命名为神经元B1-B3:

<p align="center"> 
  <img src="/imgs/czbkpics/9.png">
</p>

每一层的神经元我们让它都跟下一层的所有神经元相连接。这种连接方式我们叫做**全连接**。此时我们已经有了一个2层神经网络。当然你可以随意的复制很多层，但是要记得哦未必层数越多你的神经网络能力就会越强。

还记得老师说过我们有两种学习任务嘛？分别是关于图片的和关于语言的。它们都是应用的多层神经网络，但是很遗憾的是这两种任务不能用统一的一种神经网络结构来学习，因为这两种任务的关注点不同：

比如我们想要做一个人脸识别的应用，那么当我们把带有人脸的照片给到计算机的时候，我们希望的是计算机可以从照片中提取到属于你自己的面部特征，比如你是瓜子脸还是可爱的圆脸？你有没有高鼻梁是不是双眼皮？此时我们希望的是计算机在**捕捉局部信息**的能力上是足够强大的。

可是如果我们要做一个百度翻译的应用，我们更在意的是一个句子整体的意思，比方说“老师今天去商场买了苹果”这句话，如果我们只是逐字直译的话，苹果会被翻译成“apple”，也就是可以吃的水果。可是如果我们可以联系上下文去看的话就会发现，去超市才会买水果，去商场应该是买的苹果手机吧。此时我们希望的是计算机在**捕捉上下文信息**的能力上是足够强大的。

针对这两种任务的深度神经网络也有它们自己的名字，关于图片的叫做“卷积神经网络”，关于语言的叫做“循环神经网络”。让我们一起去了解一下吧。

## 闯关练习

1. 在神经网络中每个神经元的作用是什么？（多选题）

> A. 接收传递过来的信息
> B. 保存参数
> C. 发送整合后的信息给下一个神经元
> D. 信息整合

正确答案：A,C,D 神经元的作用是接收前面神经元传递过来的信息并且整合之后再传递给下一个神经元。


2. 在单层神经网络中，下面表述正确的是：

> A. 一层中每个神经元相互连接
> B. 有学习到最佳参数的能力
> C. 没有学习到最佳参数的能力
> D. 一定不如多层神经网络

正确答案：B 无论是几层的网络，同一层的神经元之间不相互连接；单层神经网络有学习到最佳参数的能力；神经网络的好坏不取决于有多少层，10层的神经网络未必就有1层的神经网络好。

3. 在多层神经网络中，下面表述错误的是：

> A. 每个神经元只跟下一层的某个神经元相连
> B. 网络的参数可以用梯度下降法来求解
> C. 第i层的某个神经元接收自i-1层所有的神经元传递来的信息
> D. 梯度下降法求解只能求得近似解

正确答案：A 每个神经元都跟下一层的所有神经元相连接。梯度下降法是用迭代的方式一步一步的逼近真实解，所以大多数情况下我们只能得到近似解。



## 第三关 xxxxxxx

#### 3.1 卷积神经网络


我们之前见到的多层神经网络都是前一层的神经元和下一层的所有神经元相连接叫做全连接层。在卷积神经网络中，还有另外一种连接方式叫做“卷积”。它是出现是受到了人类利用视觉神经系统看到一个物体所需要的步骤的启发。比如之前老师看到美女的过程中，“卷积”会让我提取到关键的局部信息，比如她是长头发，她是大眼睛。有了这些局部信息之后我们就不难分清我们看到的是美女还是如花。

所以卷积神经网络的卷积层可以更好的帮助计算机提高**捕捉局部信息**的能力。

那么关于语言的呢？


#### 3.2 循环神经网络

在之前“老师去商场买苹果”这句话中，我们首先翻译的是老师为teacher，其次翻译去为go to，再翻译商场为mall，可是这时候我们已有的地点信息为商场，再翻译苹果的时候就会利用前面的商场作为我们的已知。通过已知再去看看翻译成什么比较合适，所以计算机会告诉我们这里的苹果应该翻译成iphone更好。

此时与捕捉局部信息对应的，循环神经网络可以帮助我们捕捉全局的信息。在循环神经网络中，我们不停的利用已有的信息去生成新的信息。

了解完了卷积和循环神经网络，我们就可以开始动手去实现自己的深度学习模型了。我们不需要担心怎么去实现卷积和循环，甚至我们连全连接都不需要知道怎么用代码实现。因为这些固定的结构早已有前辈为我们写好了，所以做出一个自己的模型，我们只要调用前辈写好的库就够了。

现在用的最多的有两种深度学习框架为PyTorch和Tensorflow，它们都强大到可以让我们在10行代码之内搭建出深度学习模型。由于篇幅有限，我们就拿在工业上用的最多的Tensorflow来做最后的项目。


## 第四关 xxxxxx

#### 4.1 Tensorflow的安装

在安装我们的框架前，我们要保证我们已经有了python的编译环境。如果你还没有安装python，可以通过官网（https://www.python.org/）下载或者通过下载Anaconda（https://www.anaconda.com/）它会自带python环境。要注意我们用的是Tensorflow 2，所以它要求我们的python版本在3.5以上。

安装好python之后，如果你是通过官网下载的，我们可以直接在命令行输入以下命令就安装好Tensorflow啦。

```
# 如果你是通过官网下载的，就输入以下一行命令

pip install tensorflow

# 如果你是通过Anaconda下载的，下面的两行命令会为你创建一个Tensorflow的虚拟环境并且激活这个虚拟环境

conda create -n tf tensorflow            # 创建名为tf的虚拟环境，在虚拟环境下安装Tensorflow
conda activate tf                        # 激活名为tf的虚拟环境
```


安装好后，我们还需要一个代码编辑器，如果你有自己习惯的编辑器可以直接使用，如果没有偏好的话老师建议下载一个Jupyter notebook。下载Jupyter notebook也很简单：

```
# 如果你是官网下载的python，那么在命令行输入以下两行命令

pip install jupyterlab
pip install notebook

# 如果你是用Anaconda下载的，那么输入以下两行命令

conda install -c conda-forge jupyterlab
conda install -c conda-forge notebook
```

然后就完成啦，我们可以用命令行输入以下命令打开编辑器：

```
jupyter notebook
```

然后我们可以新建一个文件夹叫做deep learning然后点击右边“New” -> "Python 3"。

<p align="center"> 
  <img src="/imgs/czbkpics/10.png">
</p>

打开后我们可以检查一下是否安装成功了Tensorflow，在输入以下两行后点击上面的“Run”按钮，它就会执行当前的代码块:

<p align="center"> 
  <img src="/imgs/czbkpics/11.png">
</p>

如果运行结果跟老师的一样说明同学你已经成功配置完所有环境啦。

#### 4.2 Tensorflow项目前预备知识

Tensorflow已经为我们封装好了所有我们需要的库，包括我们的数据集也可以直接从Tensorflow下载得到。
我们要做的就是给电脑一张图片，每张图片上是手写版的阿拉伯数字0-9，我们想让计算机通过深度学习之后，帮我们预测随机的一张图片上面的数字是几。

<p align="center"> 
  <img src="/imgs/czbkpics/12.png">
</p>


好了，我们可以通过
```
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
```
来下载我们所需要的数据集。我们直接把所有数据分为训练集和测试集，训练集用来让模型训练，当模型觉得自己学习完毕之后，我们用测试集来验证一下学习效果。就好像我们的考试题库就是训练集，考试卷子就是测试集。然后我们再分别把训练集中的题目存入x_train，答案存入y_train;测试集中的题目存入x_test，标准答案存入y_test。我们总共有60000条训练集和10000条测试集。

接下来我们要为神经网络的输入做一个准备，我们知道图片都是由像素点组成的，就像用不同颜色的钉子拼出蒙娜丽莎的微笑。在我们的数据里每幅图都有28*28=784个像素点，我们现在把这些像素点拉平，形成一个一行784列的列表，就像下图一样：

<p align="center"> 
  <img src="/imgs/czbkpics/13.png">
</p>

同时我们再把每个点上的值都除以255，让它们保证在0-1之间方便计算机计算。

```
# reshape可以帮助我们得到想要的形状
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255
```

好了，我们已经准备好了输入，接下来就是激动人心的架构神经网络了！

搭积木我相信同学你肯定玩过，想搭一间屋子我们就得一块一块的转头往上垒。在Tensorflow里搭神经网络也是一模一样的方式，首先我们有一个容器，就像python的list，我们可以把数字或者字符串添加到里面。Tensorflow中我们用Sequential来容纳所有的层，用layers.Dense表示这是一个神经网络的层，那么我们如果想要这个神经网络有3个隐含层，就可以直接复制三份layers.Dense在Sequential容器内：


```
model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(32),
  tf.keras.layers.Dense(64),
  tf.keras.layers.Dense(10, activation='softmax')
])

```
layers.Dense(32)表示我们的第一层一共有32个神经元，第二层有64个。
最后我们要一个输出层，要10个神经元，对应0-9的每个数字，每个神经元都通过一个叫softmax的函数帮助我们计算了它对应的数字的概率，比如说下图中代表数字8的神经元概率最大，那预测的结果就会是8。

<p align="center"> 
  <img src="/imgs/czbkpics/14.png">
</p>

好了我们已经把神经网络搭建完了。我们离成功仅还有一步了！我们通常在备考的时候，有的同学背书，有的同学刷试卷，有的同学题海战术，目的都是查漏补缺。同样的道理，我们要希望神经网络越学越好，我们就要设置一下我们要用哪种学习方式，在这里我们使用一个叫adam的学习方式，并且通过“sparse_categorical_crossentropy”方法来告诉模型它现在离标准答案还有多远。最后我们要告诉模型你的目标是考试题目做对的越多越好，所以设置目标为准确率accuracy。

```
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

大功告成！接下来就是训练我们搭建的模型了！我们直接用fit函数，并且传给它我们训练集的题目和答案，他就可以自己学习了！

```
model.fit(x_train, y_train, epochs=5)
```

想考高分题目刷一遍哪里够？所以这里我们额外设置epochs=5，让模型自己反复的给我学个5遍！

运行！模型会实时的给我们汇报它学习的成果:

<p align="center"> 
  <img src="/imgs/czbkpics/15.png">
</p>

我们发现随着刷题轮数越多，我们的accuracy准确率也从一开始的89%提升到了91.8%。

好啦，学完之后我们就要开始考试了！我们可以从测试集里随机抽取5道题看看学习效果。这里老师为了让同学能直观的看到图片上的数字，用了matplotlib进行了绘画，如果你也想自己动手画一画不要忘记安装matplotlib哦。

<p align="center"> 
  <img src="/imgs/czbkpics/16.png">
</p>

我们随机挑选了5张图片，分别是7、2、1、0、4。我们来看看模型认不认得这些数字，用模型名.predict函数进行预测：

```
prediction = model.predict(x_test[:5]) # 给出我们之前画的5张图
```

现在prediction里面已经有了对于每个图片，它分别是0-9每个数字的概率，我们为了更清晰的看到答案，老师直接打印出来每个图片对应最高概率的是几：

<p align="center"> 
  <img src="/imgs/czbkpics/17.png">
</p>

Bingo!全对！虽然学习之后准确率是91.8%，但是对于这5张图片我们的模型还是很有自信的嘛。

好啦，现在同学可以自己对我们的模型稍加修改，比如再多几层隐含层，看看能不能超越老师。

## 课程总结

我们在本节课中了解了深度学习的概念，也了解了神经网络的组成成分--神经元。我们把很多个神经元放在一起，就变成了单层神经网络，把很多个单层神经网络放在一起就变成了多层神经网络！再后来我们知道了虽然是多层神经网络，对于不同的任务也有不同的结构。卷积神经网络可以帮助我们提取到图片的局部信息，循环神经网络可以帮助我们提取到上下文的全局信息。最后我们用当下最流行的深度学习框架Tensorflow自己手动实现了一个多层神经网络，教会了计算机认数字。

相信你已经感受到了深度学习的魅力。前路漫漫，时代在进步，深度学习的领域发展尚未完全，还有很多的未知等着我们去探索，老师欢迎你的加入！




## 备用

在每个箭头上，都有一个参数，用来表示从A1传递到B2经过怎么样的转化。

到了这里你一定会有一个疑问，之前买草莓的单价，是我们大脑算出来的。那么神经网络是如何计算这个参数的呢？

就是通过大名鼎鼎的“梯度下降法”。又懵了是不是。科学家总爱用谜语去解释一个通俗的道理。下面我来皇家翻译一下梯度下降法的基本规则：

老师又去第二次遇见美女的超市买蓝莓，照常问老板咋卖的，老板说：“20块钱1公斤，40块钱2公斤。60块钱3公斤。” 老师这回不想使用除法了，想使用梯度下降法来求解。于是老师没动脑子随口说了一个价格：“害，那就是每公斤15块钱嘛！” 老板答：“你咋算的，你这还少着几块钱呢！” 哦，老师明白了，15块钱不够，那既然少几块钱我就再多加4块钱试试吧：“害，刚逗你呢，这不就是每公斤19块钱嘛！” 老板说：“得，倒是接近了，再多个一丁点就更准了。” 老师听到了就又加了几毛钱：“好了不开玩笑了，其实应该是19块8！” 老板说：“哎就这样吧，就按这个价格卖你吧。”

在这个过程里，输入照样是20、40、60块钱，输出照样是1、2、3公斤，只是我们不知道参数不知道怎么求单价，于是我们先随机捏造一个数，尝试一下。因为老板是知道真实的单价的，所以他会告诉我们应该加多少还是减多少。在神经网络的求解参数过程中，也就是梯度下降法的步骤中，我们管“随机捏造一个价格”叫做“参数的初始化”，有了捏造的参数后我们就去告诉老板我们猜的价格，这个过程叫做“前向传播”；随后老板听到了之后算了一下还有多少差距，这个差距我们叫做“梯度”，随后给我们一个反馈，我们再去更新我们的猜测，这个过程叫做“反向传播”。经过好多轮的前向传播和反向传播后，我们刚开始凭空捏造的单价也就越来越准确了。

这就是神经网络学习参数的过程。所谓深度学习，无非就是用多层的神经网络去学习到这些参数。学到了之后我们就可以拿着这个参数去预测新的数据。
