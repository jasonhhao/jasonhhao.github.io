---
layout: post
title:  Word2Vec
date:   2020-05-25 00:00:00 +0800
categories: 自然语言处理
tag: 文本表示
---

* content
{:toc}


<h2 align="center">宏观理解</h2>

文本表示是一个很重要很重要的领域和步骤，在任何模型训练之前都要找到一个匹配的文本表示方法。本文介绍一种有全局泛化能力的分布式表示模型--word2vec。但其实word2vec是两个模型的统称，分别是skip-gram和CBOW。这两种模型及其类似，介绍完一种另一种就无师自通了，所以下面我们只详细介绍一下skip-gram。

<h2 align="center">微观分析</h2>

我们认为在一句话中，挨在一起的单词相似度是比离得远的单词相似度更高的。所以我们是不是可以通过一个词周边的词去预测这个词呢？或者说可不可以通过一个词去预测周边的单词呢？

这就是skip-gram和cbow的解题思路！

<h3>skip-gram</h3>

通过一个词去预测周边的单词！

我们现在有一个预料：We are learning NLP, it is interesting.我们假定window size为2的话，也就是说通过每一个单词，我们要去预测左边的2个和右边的2个词，比如

- We _ _ NLP, it is interesting.
- _ are _ _, it is interesting.
- _ _ learning _, _ is interesting.
- We _ _ NLP, _ _ interesting.
- We are _ _, it _ _.
- We are learning _, _ is _.

那么我们需要的就是使得P(are, learning|we), P(we, learning, NLP|are), P(we, are, NLP, it|learning), P(are, learning,it, it|NLP), P(learning, NLP, is, interesting|it), P(NLP, it, interesting|is)他们的概率都要最大化。





<h3>总结</h3>

这两种模型中运用最多的还是skip-gram，原因在于两点：

- skip-gram比cbow更加难学习，所以学出来的编码会更优秀
- 在训练之前样本可以构造的更多。








