---
layout: post
title:  图神经网络 GNN
date:   2020-07-23 00:00:00 +0800
categories: 自然语言处理
tag: 图神经网络
---

* content
{:toc}


<h2 align="center">宏观理解</h2>

图神经网络本来被应用在推荐系统、社交网络、知识图谱等，后来人们发现它还可以很好的应用的很多别的领域，所以近期这个领域大火。图神经网络顾名思义其实就是可以在图输入时用到的神经网络架构。比如典型应用有节点分类。
说到图神经网络肯定会想到CNN，但是他们的区别是图神经网络可以处理节点不固定的情况。

<h2 align="center">微观分析</h2>

首先我们定义一下图的概念。我们有一个set的节点，一个set的边。如下图所示我们用Xv来表示第v个节点上的特征；用X(v,u)来表示节点v和节点u之间的边上的特征；用hv来表示第v个节点的隐表示。这个隐表示不仅
可以很好的表示它自己，还可以加入和它相连的其他节点的信息。我们的目的就是建立一个这样的结构然后可以求的所有节点的隐表示。那么怎么做呢？

<p align="center"> 
  <img src="/imgs/gnn/1.png">
</p>

原论文中提到的一种方式是通过迭代的方式：

![](https://latex.codecogs.com/gif.latex?h_v%5E%7Bt&plus;1%7D%20%3D%20f%28x_v%2C%20x_co%5Bv%5D%2Ch%5Et_ne%5Bv%5D%2Cx_ne%5Bv%5D%29)

> 第一项表示当前节点的特征<br>
> 第二项表示与当前节点相连接的边的特征<br>
> 第三项表示当前节点的所有邻居节点的隐表示<br>
> 第四项表示当前节点的所有邻居节点的特征<br>

例如如果当前节点是x5的话，那么![](https://latex.codecogs.com/gif.latex?h_5%5E%7Bt&plus;1%7D%20%3D%20f%28x_5%2C%20x_%7B%283%2C5%29%7D%2Cx_%7B%285%2C6%29%7D%2Ch_3%2Ch_6%2Cx_3%2Cx_6%29)

那么很显而易见的是函数f处理的输入应该是变长的，因为我们根本无法确定有多少邻居和邻边。但是变长的输入总是很困难而且复杂的，我们为了使它变成定长，可以应用一下pooling，无论是多少输入，我都给你avg pooling到
一个固定的维度。

那么这个隐表示一定可以收敛吗？根据巴拿赫不动点定理，答案是肯定的。在定义f的时候会引入一个正则项，来保证每一个节点都可以收敛。

之后我们可以得到每个节点收敛的隐表示。就和语言模型预训练一样，我们可以直接在最后接一个下游任务。比如我们在做一个节点分类，看看微博账号里面哪些账号是僵尸号。在得到隐表示后我们定义一下分类网络g(hv, xv)，
接收当前节点的隐表示和特征，得出一个分类结果。如果是训练过程就在里面加一个loss，再通过反向传播来训练它。

（这里的T是迭代的步数，和时间其实没什么关系）
 <p align="center"> 
  <img src="/imgs/gnn/2.png">
</p>







