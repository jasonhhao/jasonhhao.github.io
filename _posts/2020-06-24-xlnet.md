---
layout: post
title:  XLNet
date:   2020-06-24 00:00:00 +0800
categories: 自然语言处理
tag: XLNet
---

* content
{:toc}


<h2 align="center">宏观理解</h2>

XLNet是一种语言模型，是对现有的语言模型进行了一些改进，可以更好的应用在下游NLU任务中。在介绍XLNet之前我们要先介绍一下Transformer-XL。

我们知道语言模型做的事情就是去近似一个条件概率分布，一开始使用的是LSTM，后来又有了Transformer。但是有一个问题是这些模型的输入都是固定长度的，比如512个token。那么如果我们有一个长于512的句子，就只能
做截断。比如我们会把句子分割成每4个token的截断，然后分别把这4个token喂给模型。我们用第一个token来预测第二个，用第一、二个来预测第三个。。。这样循环下去，我们就有了4个loss，然后在优化的时候我们把4个loss
的总和进行反向传播。然后再次进行下一个4个token的截断。

这样一听就明显有几个问题，首先就是每个截断之间的关联没有了。如果句子中的第五个词和第一个词有一定关系，这样截断的话他们的关系就不能被模型注意，所以斩断了长序依赖。
在预测的时候同样有一个问题，我们在预测第五个词的时候，模型根本看不到前面的4个，只能瞎猜。
还有就是我们在evaluation中预测第4个词的时候可以把1-3放到模型中，可是如果要预测第5个词，我们又要重新把2，3，4放在模型中，因为前后截断的隐向量是不可见的。

所以Transformer-XL想做的事情就是把前面的截断的隐状态可以传递给后面的截断。

<h2 align="center">微观分析</h2>

在RNN时代，我们当然同样面临这个问题。人们就会用Truncated BPTT来解决这个问题。也就是我们把第四个token的隐向量传递给下一个截断的第一个，通过这种方式来传递之前的信息。但是在第二个截断进行反向传播
的时候，我们不对第一个截断进行回传，所以叫做Truncated。

我们当然可以把这种套路应用在transformer里面。


<p align="center"> 
  <img src="/imgs/xlnet/1.png">
</p>

我们在计算第五个词的时候，我们可以缓存下来前面的隐变量，然后利用self attention的机制让第五个词可以直接看到这些隐变量，反向传播的时候我们照样可以对前面的进行停止传播。stop gredient下面公式用SG表示。

<p align="center"> 
  <img src="/imgs/xlnet/2.png">
</p>

第一步是我们把前一个截断的context和当前截断的context拼接起来，然后在self attention的时候用当前位置的context隐向量去做query，用拼起来的向量去做key和value来计算下一个位置的隐状态。

除了




