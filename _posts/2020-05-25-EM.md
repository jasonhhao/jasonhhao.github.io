---
layout: post
title:  EM算法
date:   2020-05-25 00:00:00 +0800
categories: 自然语言处理
tag: EM算法
---

* content
{:toc}


<h2 align="center">宏观理解</h2>

EM算法（Expectation-maximization algorithm）是用来在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐变量，比如说HMM隐马尔科夫模型。
EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以被称为EM算法。
基本思想：
1. 根据己经给出的观测数据估计出模型参数的值
2. 根据上一步估计出的参数值估计缺失数据的值
3. 根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计
4. 返回到步骤1，直至最后收敛，迭代结束。

<h2 align="center">微观分析</h2>

你跟一个导师在做一个社会调研，课题是研究不同城市的男女身高分布。你手上现在有其中某一个城市收集出来的数据，一共有1000条，500条男性身高数据，500条女性身高数据。
导师突然走进来说：“你把这些数据统计一下吧，然后发布一个正态分布的模型。”
你说：“好，那这个模型的参数是什么？”
导师说：“算算咯，你可以用一下最大似然估计。”

<h3>最大似然估计</h3>

最大似然估计应用的情景同上，当我们有了两个已知条件（1）样本的概率分布（2）样本，就可以求出来分布模型的参数。
在导师给你的资料中，我们首先确定了它是服从正态分布的，我们又知道样本，那么此时我们缺的就是正态分布的参数也就是均值和方差。
在导师给你的问题中，每条男女生的身高数据是独立的，也就是说数据间互不影响，所以

那么按照最大似然估计求解参数的步骤
1. 我们需要写出一个最大似然函数，
> ![](https://latex.codecogs.com/gif.latex?P%20%3D%20%28x_1%2C%20x_2%2C%20...%2C%20x_n%29%20%3D%20f_d%28x_1%2C%20x_2%2C%20...%2C%20x_n%7C%5Ctheta%20%29)



首先明确一下两个符号，通常我们把隐变量用z来表示，隐变量就是不能观测到的数据；把观测变量用y表示，观测变量就是已知的可以观测到的数据。举个例子：
我们有10000个志愿者的身高数据，想要估算男性的身高分布N(μ1,σ1)和女性的身高分布N(μ2,σ2)。这里，性别数据未观测到，但我们仍然需要对不同的性别做估计。这里的性别特征就是隐变量，
相对应的，身高数据就是观测变量。

其中，

> complete case：(z,y)两个变量都可以被观测到<br/>
> incomplete case：(z)不可以被观测到但是(y)可以




















