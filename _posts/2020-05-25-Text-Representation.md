---
layout: post
title:  文本表示
date:   2020-05-25 00:00:00 +0800
categories: 自然语言处理
tag: 文本表示
---

* content
{:toc}


<h2 align="center">宏观理解</h2>

我们在文本预处理完之后就要喂给机器进行学习了，那么肯定不能直接给，我们怎么让机器能学习这些文本呢？就要把这些文本进行
一些编码，我们要争取让这些编码所含的信息越多越好，比如相同的

<h2 align="center">微观分析</h2>

文本表示的发展也是有一个路径的，从一开始的独热编码到现在的深度学习编码，下面介绍一些文本表示的发展路程。

<h3>独热编码</h3>

没啥好说的，现在也不会再有人去用这种表示方法了，它仅仅是把一个词表进行独热编码，也就是说每个单词的纬度就是这个词表的长度，然后除了在相应位置上设为1之外，
其余都是0。那么就有了很多问题，最直观的两个就是这个编码太大了，一个词库的长度可能要有几万十几万，那么一篇文章有100个词就有一个100*100000的矩阵，如果是是
1000000篇文章进行学习呢？第二个是稀疏性，机器学习里面稀疏是很可怕的一个词，它会让所有内积为0，失去任何意义。还有就是这种编码体现不了单词的频率，出现频率越多
的词也许在某篇文章中是关键词，那么怎么把词频也给加进去呢？

<h3>词袋模型 Bag of words</h3>

我们可以把所有的单词，也就是整个词表，打乱顺序后扔到一个大袋子里面。然后给一句话编码的时候，其纬度也一样是词表的长度，从index = 0开始，如果词表中的
这个词在句子中出现过几次，就标为几。
- 词表 = 【我们，你们，他们，去，来，也，到，这里，想，不想，那里，哪里，公园，草坪，博物馆】<br/>
句子 = 【我们，想，去，博物馆，也，想，去，公园】<br/>
文本表示 = 【1，0，0，2，0，1，0，0，2，0，0，0，1，0，1】

我们在有每个句子的表示编码之后，可不可以求得两个句子的关系/相似度？最常用的有余弦相似度，欧式距离等。但是我们马上又发现了一个问题，就是在上例中的“去”是2，
而“公园”和“博物馆”却是1，那么可以说“去”比“公园”要更重要吗？当然不是，那我们就要想个办法怎么去把单词的重要程度表示出来呢？

<h3>TF-IDF</h3>

人们认为一个词的重要性体现在两个方面，一个是在这个单词在文本中出现了多少次，我们认为出现越多次可能会越重要，也就是由tf来衡量；另一个是这个词在多少个文本中出现过，如果这个词在几乎所有
的文本都出现过，那么我们就会降低它的重要程度，由idf来衡量。我们在idf中加入log是因为增加计算的稳定性，降低由于数量相差太大引起的巨大影响。

<p align="center"> 
  <img src="/imgs/textrepresentation/1.png">
</p>





















