---
layout: post
title:  贝叶斯模型Bayesian Model
date:   2020-08-01 00:00:00 +0800
categories: 自然语言处理
tag: 贝叶斯模型
---

* content
{:toc}


<h1 align="center">宏观理解</h1>

为什么贝叶斯模型在各种领域里地位很重要？

1. 因为LDA模型在主题模型里面起到非常重要的作用。LDA模型是无监督学习模型，可以当作是Mix-membership models的一种，作用是如果给定一篇文章属于多个主题，它可以生成每个主题的概率
分布，而如果用朴素贝叶斯则只能生成一个主题类的预测，也可以叫做Uni-membership model。

2. 用于小数据量的学习问题。因为小数据量很容易造成过拟合，那解决的方法可以是集成模型，而贝叶斯模型本身也是一个集成模型。而和传统集成模型比如随机森林集成有限个模型不同的是，它集成的是无限多个模型。
但是在数据量比较大的时候贝叶斯模型学起来会很慢。

3. 把不确定性融合在模型本身。

4. 把先验融合到模型中。

5. 模型压缩。



<h1 align="center">微观分析</h1>

贝叶斯是一个很大的领域，所以我们按以下的顺序进行剖析：
1. 什么是贝叶斯？
2. 贝叶斯的推理     
      2.1 MCMC采样
      2.2 变分法variational inference
3. xxx


<h3>1. 什么是贝叶斯？</h3>

说到什么是贝叶斯，我们就要理清楚MLE、MAP和Bayesian的区别。这三个都是构造目标函数的方法，当一个模型分别用这三种方式构造时，形成的模型也是不同的：

<p align="center"> 
  <img src="/imgs/bayesianModel/1.png">
</p>

首先最简单的MLE，我们在训练的时候是想求出一个最好的参数Θ，然后使用这个Θ来做预测；MAP和MLE很相似，不一样的是MAP可以理解为加了正则项后的MLE，也可以理解为多了一个先验的MLE，这个先验也会影响最后学出来的Θ值；最后这个Bayesian有些不同，如果说MLE和MAP只是从所有可能取值的Θ中找出最佳的一个Θ，那么贝叶斯做的事情就像是集成模型，不抛弃任何一个Θ，反而让所有的Θ都用来预测最终结果。例如上图中的训练过程，我们会给每一个预测加一个权重，然后学习这个权重。预测过程也是这个套路，首先我们要P(Θ|D)估计一下参数分布，然后在某一个特定的参数下乘以前面的部分就是它的期望，如果是连续的我们就求积分，如果是离散的我们就做加法。


<h3>2. 贝叶斯的推理</h3>

我们上一步已经得到了一个预测过程的公式，可以看出来我们主要要解决的就是后验概率估计P(Θ｜D)，我们不妨先用贝叶斯公式列出来看看：

![](https://latex.codecogs.com/gif.latex?P%28%5Ctheta%7CD%29%20%3D%20%5Cfrac%7BP%28D%7C%5Ctheta%29P%28%5Ctheta%29%7D%7BP%28D%29%7D%20%3D%20%5Cfrac%7BP%28D%7C%5Ctheta%29P%28%5Ctheta%29%7D%7B%5Cint_%7B%5Ctheta%7D%5E%7B%7DP%28D%2C%5Ctheta%29d%5Ctheta%7D%20%3D%20%5Cfrac%7BP%28D%7C%5Ctheta%29P%28%5Ctheta%29%7D%7B%5Cint_%7B%5Ctheta_1%7D%5E%7B%7D%5Cint_%7B%5Ctheta_2%7D%5E%7B%7D...%5Cint_%7B%5Ctheta_d%7D%5E%7B%7DP%28D%2C%5Ctheta%29d%5Ctheta_1%20%5Ctheta_2...%5Ctheta_d%7D)

但是最后这个分母没有办法计算啊。那既然没有办法得到一个确切的值，估计一个差不多的也可以。这就是贝叶斯的approximate inference。

<h3>2.1 MCMC采样</h3>


既然积分的计算是无穷的，那么我们可以直接随机采样让它变成有限的，这样问题就解决了。这个随机采样算法我们可以用蒙特卡洛Monte Carlo采样算法。可如果是随机采样，我们无法保证最终的答案趋向于最优解，因为我们很有可能采样了很多很差的可行解，那么怎么处理这个问题呢？Markov Chain Monte Carlo简称MCMC算法可以帮到我们。它不再是随机采样，而是首先找出比较优的一个可行解，然后在这个解的周围寻找下一个可行解。因为它有一个概念基础就是，一个优秀的人周围更大可能存在同样优秀的人。所以按照这个思路，我们采样的n个可行解可以比随机抽样更加趋向于最优解。

<h3>2.2 变分法variational inference</h3>

除了积分是无穷的我们可以强制采样让它变成有限的以外，还有另一种也很直接的方法就是我们把积分转化成一个更简单的形式，使这个更简单的形式的解无限接近此时的积分。











