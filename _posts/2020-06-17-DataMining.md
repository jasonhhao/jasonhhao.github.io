## 课程导学

🎤You’re here~ there’s nothing I fear~ and I know that my heart will go on~

同学你好，我是Jason老师。1912年泰坦尼克号的沉没至今都会牵动人心，伴随着这首泰坦尼克号主题曲《My Heart Will Go On》，接下来的几十分钟老师想带你用【数据挖掘】的角度再来回顾一遍经典。

很多人会称那些幸存者为幸运儿，可他们的幸存真的仅仅是靠的运气嘛？在发生灾难的时刻哪些人更容易存活下来？老师用数据挖掘的方法从泰坦尼克号所有人的登记信息记录中找到了一些规律，证明了那些幸存者可不单单只是因为幸运哦。想知道老师都分析出来了什么嘛？让我们一起去看看吧！

## 第一关 案例分析

#### 1.1 案例展示

老师手里有一份关于泰坦尼克号的数据集，同学可以点击[这里](https://www.kaggle.com/c/titanic/data)下载。里面有891个人的登船记录，每个人都有以下信息登记在册：

<p align="center"> 
  <img src="/imgs/pics/1.png">
</p>

我们这节课就是想从这891份数据中挖掘出潜在的知识 -- 谁可以幸存？

通过这个案例，我们可以对数据挖掘的流程和数据处理细节都有一个宏观的了解，这样当别人谈起来数据挖掘你也可以侃侃而谈啦～

在我们开始之前老师想先阐述一下数据挖掘是个什么东西？都有哪些步骤？都有什么工具？

#### 1.2 啥叫数据挖掘

如果老师告诉你，在你家后院的树底下有宝藏，你会不会两眼放光抄起铁锹就去了？呼哧呼哧挖了一夜挖出来一个大元宝～在你挖宝的这个过程中有几点值得注意：

- 这个宝藏是被藏起来的，不是那么容易被人发现
- 你挖出来元宝之前并没人知道是什么
- 这个宝藏是有价值的，可以利用的
- 挖宝的过程需要用到你的智慧，比如从哪开始挖怎么挖

数据挖掘也是一样的，只不过现在你家后院变成了大量的数据，你的铁锹变成了模型，你最终挖到的宝藏变成了隐藏在数据里有价值的信息。所以我们在说【数据挖掘】的时候，我们在挖掘的宝藏--数据，有以下四个特点：

- 这个数据是隐式的，不能从表面就让你看出来
- 你挖出来之前这个数据是不被人知道的
- 这个数据是有价值的，可以利用的
- 挖掘的过程是不容易的，要体现一定的智能性

那么一般的挖掘过程是怎么样的呢？


#### 1.3 挖掘流程和工具

首先当然我们要有数据，所有任务的第一步都是收集数据。有了数据之后还不赶紧看看长得啥样，我们可以利用各种可视化库来观察一下数据的内容，比如matplotlib或seaborn。

通过可视化我们可以发现数据里面有没有“奇怪”的东西～我们叫【异常值】。异常值包括格式有问题的数据，例如年龄信息填的不是数字，或者信息根本就不符合逻辑，比如年龄信息填的200岁。

同学填过各种调查问卷吧～你是不是也跟老师一样不是必须填的就空着。。这就导致数据集里除了异常值，还有一个经常会遇到的就是【缺失值】。我们也会通过一些手段来弥补一下这些空缺。

好啦，剔除完那些奇怪的东西之后我们拿到了一份干净的数据，我们就要准备一下如何把这些数据喂给各种模型。在数据挖掘中常见的模型翻来覆去就那么几个，比如决策树，线性回归，梯度提升树，k-means等。

最后挑选两三个顺眼的模型看看结果怎么样，对比一下谁比较强～

所以在我们整个的案例中，分析流程就是这样的：

<p align="center"> 
  <img src="/imgs/pics/2.png">
</p>

当然啦，人类都是图方便的，市面上已经有很多的数据挖掘软件可以供我们使用，几乎不需要写任何代码，例如Orange、Weka等。
在我们这次案例中，老师还是用python来手写一步步的过程，然后同学想再试用一下那些软件就会发现异常的简便～

好啦，话不多说，让我们开始挖掘吧！

## 第二关

#### 2.1 加载数据集并了解数据集

老师要用自己很热爱的编辑器jupyter notebook～同学也随意挑选自己喜欢的就好。

我们下载好数据集之后，第一步就是导入数据：

```
import pandas as pd

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
```

官方给的数据分为训练集（train.csv）和测试集（test.csv）。我们都分别导入一下来看看都有什么内容～


<p align="center"> 
  <img src="/imgs/pics/3.png">
</p>

在训练集里面一共有891行，每一行对应一个人的数据。测试集有418行，还少了一个是否幸存的Survived列是我们需要通过模型来预测的。我们可以简单看一下训练集的内容：

- PassengerId：就是一个0-890的编号，没啥意义不用理会。
- Survived：这名乘客是不是幸存了，如果活下来了就是1，死亡了就是0。
- Pclass：可以想像成飞机的头等舱商务舱经济舱，分别用1、2、3来表示
- Name：乘客名字，看样子有一些名字还有类似“先生/女士/太太”的后缀
- Sex：male就是男性，female就是女性
- Age：嗯。。年龄
- SibSp：船上兄妹和伴侣的人数
- Parch：船上父母和子女的人数
- Ticket：票号，就像一堆乱码
- Fare：票价，看样子票价挺不统一的
- Cabin：在哪个船舱，是一个船舱的编号
- Embarked：在哪个港口登的船，有三个，分别用S、C、Q表示

除了直接打印出来，我们还可以通过下面两个方式查看一下更加细节的东西：

```
train.info()

train.describe()
```


<p align="center"> 
  <img src="/imgs/pics/4.png">
</p>

info()函数帮助我们总结了都有那些列，然后这些列中有多少个是非空的。我们发现Age里面有大约不到200个缺失值，Cabin更严重。。只有204个有值，这些我们都会在后面进行一个处理。

describe()函数帮助我们统计了一下数值列的个数啊均值啊方差啊最小值最大值什么的。我们可以看到最小的年龄是0.42。也许代表刚出生几个月吧～

好啦，现在我们宏观的了解了一下我们的数据，接下来我们利用可视化工具来深入到某些列看看还隐藏着哪些不为人知的秘密呢～

#### 2.2 对数据进行可视化

我们最感兴趣的当然就是是否生还啦，所以我们第一步先可视化一下Survived这一列。可以通过value_counts来看一下百分比，再来用seaborn画出来～

```
train['Survived'].value_counts(normalize=True)  # normalize=True 计算百分比

import seaborn as sns
sns.countplot(train['Survived'])
```


<p align="center"> 
  <img src="/imgs/pics/5.png">
</p>

umm..活下来的人占了38%，比老师想象的要多很多啊！

好啦，再来看一下Pclass列。来看看是不是有钱能使鬼推磨！我们根据是否幸存和在哪个等级的船舱整合起来画一下：

```
sns.countplot(train['Pclass'], hue=train['Survived'])
```

<p align="center"> 
  <img src="/imgs/pics/6.png">
</p>

横坐标表示分别在头等舱、商务舱还是经济舱，纵坐标表示人数，橘色表示生还人数，蓝色表示死亡人数。

可怕不！事实证明关键时刻人家头等舱的人就是比我们经济舱的人幸存几率大啊！

我们接下来看一下Name这一列。同学是不是想名字这种天生的东西能看出啥。别急，我们来看看。最直白的方法是我们统计一下名字的长度，看看是否幸存，和名字的长度有没有关系：

```
train['name_len'] = train['Name'].apply(lambda x: len(x))
train['Survived'].groupby(pd.qcut(train['name_len'], 5)).mean()
```

我们可以创建新的一列，就叫name_len 名字的长度，然后根据这个长度我们和是否幸存做一个统计。

<p align="center"> 
  <img src="/imgs/pics/7.png">
</p>

有没有毛骨悚然，数据告诉我们，如果你的名字字符在32-82之间，你的幸存概率是67%。随着名字长度越长，生还的概率越大！顿时羡慕维吾尔族的兄弟们有没有。其实从某一个方面也可以说得通，就是数据集里面的名字都是带着头衔的，比如博士、商务部长、蒙面黑衣人啥的。。也就是说如果你的头衔越长。。可能你的地位越重，那么你可能才会得到更多的生还机会～

接下来我们再来看性别和幸存的关系

```
train['Sex'].value_counts(normalize=True)
train['Survived'].groupby(train['Sex']).mean()
```

<p align="center"> 
  <img src="/imgs/pics/8.png">
</p>

嗯。。可见74%的女性都幸存了，而只有18%的男性幸存。这也说明了当时还是禀着女士优先的习惯让女性先逃生。

接下来我们看一下票价的影响。其实可以想象到既然头等舱的幸存率更高，那么肯定也意味着船票越贵的生还几率就更大一些，我们来看看是不是这样：

```
train['Survived'].groupby(pd.qcut(train['Fare'], 5)).mean()
```

<p align="center"> 
  <img src="/imgs/pics/9.png">
</p>

嗯果然如此啊。看来今后出门还是不能太扣啊。

我们最后来看一看年龄的影响，我们把年龄按年龄段分成5组：

```
train['Survived'].groupby(pd.qcut(train['Age'], 5, duplicates = 'drop')).mean()
```

<p align="center"> 
  <img src="/imgs/pics/10.png">
</p>

19岁一下的孩子幸存是最高的，应该当时都是妇女儿童先走～20多岁的小伙子幸存最低啊，大概都舍己为人了吧。30岁往上的估计好多都是带着长长的名字和买了贵贵的船票才拉高了幸存率吧。。

同学可以自己手动模仿老师对剩余的几列进行一下可视化，看看他们背后还隐藏着什么。

大致看完了这些人情冷暖我们就要处理一下数据里面的脏东西了。

## 第三关 数据预处理

#### 3.1 缺失值和异常值

我们在之前观察train的时候发现有3列是存在缺失值的，分别是Age、Cabin和Embarked。别忘了我们的test训练集也是需要处理的，我们分别看一下train和test的数据缺失情况：

<p align="center"> 
  <img src="/imgs/pics/11.png">
</p>

现在这个时代什么最贵？数据最贵！所以我们当然希望能补全的数据就不要轻易扔掉，那我们就要想，像年龄这一列怎么来补全呢？都强制设置成统一的数字？那么怎么选取这个数字呢？我们可以用所有其他年龄的均值？或者哪个年龄最多我们就用哪个？再或者更麻烦一点我们通过其他的列来建模预测一下年龄？当然都是可以的，这里我们就设置成均值好啦：

```
# 填充所有空值为train/test的年龄这一列的mean均值
train.Age.fillna(train.Age.mean(), inplace=True)
test.Age.fillna(test.Age.mean(), inplace=True)

train.info()
```

<p align="center"> 
  <img src="/imgs/pics/12.png">
</p>

我们看到年龄Age这一列已经被填满了，接下来我们想一想Cabin怎么办？这也太少啦，缺了一大半啊。那我们干脆扔掉好了～

```
train.drop(['Cabin'], axis = 1, inplace = True)
test.drop(['Cabin'], axis = 1, inplace = True)

train.info()
```

<p align="center"> 
  <img src="/imgs/pics/13.png">
</p>


嗯。。很好，现在Cabin这一列已经消失掉了。还有最后一个Embarked，只缺了两个。那么我们就看哪个港口登船的最多就把它们设置成哪个港口吧～

```
train['Embarked'].value_counts()

output：
S    644
C    168
Q     77

```

好，既然S港口这么多，估计缺失的两个大概率也是从S港口上的船，那就把缺失值设置成S：

```
# test里面是满的，只填充train就好
train['Embarked'] = train['Embarked'].fillna('S')
```

我们在test里面还有一个票价Fare缺一个值，那就直接像年龄一个计算一个均值给它吧：

```
test.Fare.fillna(test.Fare.mean(), inplace=True)
```

到目前为止我们的数据都已经被填充满啦，这样的数据看起来舒服多了

<p align="center"> 
  <img src="/imgs/pics/14.png">
</p>


现在虽然数据已经被填满了，可是里面会不会有一些异常值呢？

我们通过train.describe()和test.describe()简单观察一下

<p align="center"> 
  <img src="/imgs/pics/15.png">
</p>


看样子这些数据还算老实，没有什么负20岁或200岁的，兄弟姐妹子女父母的数量也都还可以接受。那我们就保持不动吧～如果同学以后在项目中发现了一些异常值也可以把他们当成空缺值进行一个替换。


#### 3.2 特征选择

在我们的数据都清洗干净之后，在喂给模型之前，我们要做一个决定！就是这么多列的信息我们是不是都需要？
我们想做的是预测出来训练集中的每一个人是否会幸存，那么通过刚才的观察，是否有一些列并不会对结果造成影响？

我们可以通过corr()来看一下训练集里面的数据相关性，

```
train.corr()
```

<p align="center"> 
  <img src="/imgs/pics/16.png">
</p>

当然啦这里面只有数值型的列，字符串的格式也没办法统计嘛。我们可以看到蓝框圈起来的关于Survived和其余的列的相关程度。和我们之前可视化分析的内容差不多，Pclass和Fare都是正相关性比较大的，意味着钱越多你的生存机会就会越大。比较少的是SibSp（兄妹和伴侣的人数），可以考虑把这一列给丢掉。

除了这些我们还有几个字符串的列，比如PassengerId、Ticket和Name。乘客编号和票号是随机的嘛，我们可以默认为关系不大。名字这一列我们已经为它额外提取了一列"name_len"了，所以名字这一列我们也可以不要了。

```
train.drop(['Ticket','Name', 'SibSp', 'PassengerId'], axis = 1, inplace = True)
test.drop(['Ticket','Name', 'SibSp', 'PassengerId'], axis = 1, inplace = True)
```

好了，到此为止我们的数据算是处理完成了，接下来我们需要把这些数据喂给模型。

可模型也是有脾气的，不是你想喂啥模型就愿意吃的，那我们为了伺候好，在喂之前还需要一个把数据稍微的加工一下。


## 第四关 数据转化

#### 4.1 独热编码

千万不要被这些高大上的名称给吓到，其实是很简单的过程。我们想一想，像Embarked这种列，我们给到模型一堆S、C、Q有啥意义呢？

模型会抱怨你给我的这啥叫S啥叫Q啊？！

为了安抚模型，我们对于这种含有字符串的数据，给转化成数字，这个转化的过程我们可以使用独热编码来完成：

<p align="center"> 
  <img src="/imgs/pics/17.png">
</p>

在这里一共有3种颜色，我们把每个颜色对应的一列上的数字标为1，其余为0。这就是独热编码。相同的，如果我们想编码我们的港口编号，也是一样的操作：

<p align="center"> 
  <img src="/imgs/pics/18.png">
</p>

现在我们就可以把Embarked这一列替换成右边的一个3✖️3的矩阵。也就是意味着我们要丢掉Embarked这一列，并且再添加【Embarked_S, Embarked_C, Embarked_Q】三列。我们同样对性别和船舱的等级也都做一下独热编码：

```
def dummies(train, columns):
    for col in columns:
        train[col] = train[col].apply(lambda x: str(x))
        new_cols = [col + '_' + i for i in train[col].unique()]
        train = pd.concat([train, pd.get_dummies(train[col], prefix=col)[new_cols]], axis=1)
        del train[col]
    return train

train = dummies(train,['Sex', 'Embarked', 'Pclass'])
test = dummies(test, ['Sex', 'Embarked', 'Pclass'])
```

这样我们的数据集就变成了这个样子，我们仍然可以看出第一个人是男性，在S登船，在经济舱。

<p align="center"> 
  <img src="/imgs/pics/19.png">
</p>

完成！现在模型就不会再抱怨它看不懂了。所以对于我们数据中的离散型数值（非连续的）我们就可以使用独热编码来加工一下让模型更好理解。那么同学到这里会不会有一个疑问，那么如果是连续的数值，是不是就不需要额外加工了？

理论上说是这样的，可是为了模型能得到更好的结果，我们希望数字1和10000对于模型来说不要有那么大的差距，所以我们可以用分桶来处理一下。

#### 4.2 分桶

也是异常简单的机制，就是把我们的数据平均分成k份，例如我们把年龄分成【0-20，21-40，41-60，61-80】这四个区间，那么如果你的年龄是36岁，此时你的年龄这一列就会被改成2，意味着你在第二个区间。

我们接下来对年龄做一下分桶，记得测试集也要改：

```
train.loc[(train['Age'] <= 20), 'Age'] = 1
train.loc[(train['Age'] > 20) & (train['Age'] <= 26), 'Age'] = 2
train.loc[(train['Age'] > 26) & (train['Age'] <= 30), 'Age'] = 3
train.loc[(train['Age'] > 30) & (train['Age'] <= 40), 'Age'] = 4
train.loc[(train['Age'] > 40), 'Age'] = 5

test.loc[(test['Age'] <= 20), 'Age_new'] = 1
test.loc[(test['Age'] > 20) & (test['Age'] <= 26), 'Age'] = 2
test.loc[(test['Age'] > 26) & (test['Age'] <= 30), 'Age'] = 3
test.loc[(test['Age'] > 30) & (test['Age'] <= 40), 'Age'] = 4
test.loc[(test['Age'] > 40), 'Age'] = 5
```

好啦，同学自己可以尝试用相同的方法对票价和名字的长度做一下分桶。这里篇幅有限就不多赘述了。
到此为止我们的数据已经加工好可以喂给模型了，接下来我们凭借心情随机选两个模型看一看效果。

## 第五关 建模

#### 5.1 训练模型

为了达到一个比较好的训练效果，老师在之前的数据预处理的基础上多做了一些工作。我们这节课的关键是了解一下整体流程，所以如果同学自己训练的模型效果不理想也不要苦恼，我们这节课并不是主旨在训练一个好模型。

我们首先从训练集中分离出训练的数据和标签，然后把891份数据分成两份用来做训练和验证：

```
train = train.iloc[:,1:] # 从第1列开始往后用来训练
label = train.iloc[:,0]  # 第0列为Survived，作为标签进行预测

trainX, testX, trainY, testY = train_test_split(train, label, test_size=0.2, random_state=1)
```

今天老师心情不错，决定选逻辑回归和决策树。在sklearn中我们可以直接调包训练～

```
from sklearn.linear_model import LogisticRegressionCV

lr = LogisticRegressionCV(cv=5, random_state=0).fit(trainX, trainY)
lr.score(trainX, trainY)

输出：
0.7303370786516854
```

我们可以看到得分为0.73，说明有73%都预测对了。我们再来用决策树来试一试：

```
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=0)
dt = dt.fit(trainX, trainY)
dt.score(trainX, trainY)

输出：
1.0
```

哇决策树竟然可以学习到100%的正确率。但是我们这些分数都是在训练集上，我们接下来放到测试集上看看两个模型的效果：

```
lr.score(testX, testY)

输出：
0.659217877094972
```

```
dt.score(testX, testY)

输出：
0.770949720670391
```

嗯。。看来决策树比逻辑回归训练的要优秀一点。

## 总结与拓展

我们这节课通过数据挖掘的方式，揭开了泰坦尼克号里的人情冷暖。闭上眼睛回想一下我们这一路分析过来，都用到了哪些方法和技巧？

<p align="center"> 
  <img src="/imgs/pics/20.png">
</p>

首先我们对数据进行了一个整体的观察，看出幸存率大概在30%多。随后我们又对每一列的数据进行了可视化和统计，领悟到了以后出门有钱就买头等舱=，=

随后我们对数据里面的缺失值进行了填充，也检查了一下有没有异常值。接下来我们傲娇的模型不认识字符串，我们又辛苦地对离散数据进行了独热编码，对连续数据进行了分桶。

最终喂给了两个模型，对比了一下在测试集上的结果。

我们这个整体的流程，就是数据挖掘的经典流程。我们常说在一项数据挖掘或者机器学习的任务中，最重要的就是对数据的了解和如何进行预处理。那么老师想拓展一下，给同学留个兴趣作业：

1. 我们有没有更好的方法找到数据中的异常值？
2. 我们在选择把那些列喂给模型时，有没有更准确的选择方式？

相信同学已经有能力独自掌握这些问题的答案了～
