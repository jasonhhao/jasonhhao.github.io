## 课程导学

同学好，我是Jason老师，今天老师的任务就是带领同学入门机器学习这个领域。机器学习是人工智能里的一个分支，也是深度学习的基础学科，如果将来想从事关于人工智能或者大数据分析相关的工作，那么就一定要在机器学习上有一个夯实的基础。

在这节课程中，我们会从介绍什么是人工智能和机器学习的基础知识开始，再通过深入机器学习的算法黑盒让同学有一个全面的概念，最后我们要通过课上讲解的机器学习算法对房价进行预测。

当同学上完本次课程后，你会：

- 了解人工智能领域的分类
- 了解机器学习的概念和分类
- 了解机器学习建模过程
- 了解机器学习常见模型
- 掌握使用开源框架建模机器学习项目

话不多说，让我们开始吧！

## 第一关 揭开神秘面纱

#### 1.1 机器学习简介

想象一下你正走在街上，迎面跑来一个穿着西服的小伙子，我们可以判断他也许是在赶公交车上班，也许是公司就在附近但快迟到了。这是我们大脑对这件事情的预判，我们的预判是通过经验得来的，因为我们没见过穿着西装做运动的。
那么机器是不是也可以做这个预判呢？这个就是机器学习这个领域研究的内容，我们意图使用机器来模拟人类的思维处理过程，来得到和人类相似的甚至超越人类的处理结果。这个结果可以帮助我们对未来进行预判、发现人类难以观测到的细节、帮助我们做可行性最高的决策等等。

想象一下有个穿着西服的男生在街上跑步冲进旁边的一栋写字楼，你会不会觉得这个画面毫无违和感？这个就是我们的经验，也许是我们曾听说过某某在跑来公司的路上，也许是我们确实亲眼见过。
在机器里，这种经验通常是用数据来存储的。我们可以通过使用这些经验来生成一个模型，这个从经验到模型的过程，我们称之为“学习的过程”。
那么显而易见，机器学习就是研究如何从经验中得到模型，也就是，如何“学习”？

其实机器学习早在20世纪50年代就已经有了概念，只不过当时还没有形成体系，那时候的机器学习被人类用来证明一些数学定理，更多是停留在“逻辑”层面。后来到了70年代，人们发现如果真的要实现智能，那么“知识”是不可或缺的，仅凭“逻辑”是无法实现真正的智能的。但是我们把“知识”输入到机器里面之后，机器便可以成为所有领域的专家，它可以帮助我们在一些问题上做推断，但是马上就迎来了下一个瓶颈：机器并不会自己产生新的知识。那这个就很麻烦了，换句话说，一个只有逻辑推理和知识库的机器，永远不能提高自己的智能水平。一直到了近2000年，随着硬件系统的性能提升和互联网推动的数据量爆发，让人们有了可以“玩转”机器学习的资本，人们才把机器学习设置成一个独立的体系，随后便有大量的研究员涌入这个新潮的领域。

短短的小几十年时间，我们已经可以看到各种人工智能充斥在我们的日常生活中。

#### 1.2 现实生活中的应用

我们已经可以说是生活在人工智能时代的人类了，手边随手一抓都是智能的产物，例如手机上的指纹解锁、人脸识别。还有各种购物软件上的“猜你喜欢”：

<p align="center">
  <img src="pics/1.png">
</p>

除了商业上，在医学上还被用于核磁共振影像的鉴别，这样就避免了因为医生个人“眼花”导致病人耽误病情的发生。

<p align="center">
  <img src="pics/2.jpg">
</p>

在金融领域，最耳熟能详的莫过于股票市场走势的预测或者进出口货物的交易量预测：

<p align="center">
  <img src="pics/3.png">
</p>

还有比如在房地产业，我们本节课的实战项目，也是很多年前一个真实的案例，使用的是美国华盛顿州King县的房产真实信息，预测某个地段某些条件的房屋售价。

看了这个多的人工智能成品，想不想接下来揭开机器学习神秘的外衣？


## 闯关练习

1. 当代机器学习模型学习的根据是：

A. 经验
B. 逻辑
C. 规则

答案：A，机器学习模型的演变是从规则和逻辑到如今的依靠经验。



## 第二关 透过表象看本质

#### 2.1 专业术语解释

磨刀不误砍柴工，在进入模型的介绍之前，我们要先了解一下在机器学习建模中会用到的“黑话”，这样使后续的讲解和实战更有效率。
举个例子，我们现在有一个后面实战项目的迷你版本，根据[几室，是否是学区房，在低/中/高楼层]来预测房屋的价格。

我们的手头上有100条类似数据：
- [2室，非学区，低层] = 50万
- [1室，学区，高层] = 84万
- [2室，学区，中层] = 97万
- ...

我们管所有这些经验数据的集合叫做【数据集】，所有我们数据集的大小为100；我们管数据集中的每一条数据叫做【样本】，所以我们有100条样本；在每一条样本中，我们有三项预测根据[几室，是否是学区房，在低/中/高楼层]，我们管每一项叫做一个【特征】；在样本中除了特征，还有预测的价格，我们叫这些已知的价格为【标签】。

我们人类在学习和考试的全过程中需要两套题，一套题叫练习题，另一套题叫考试试题。所以我们也给机器两套题。例如上面的100条房屋数据，我们从中抽取70条样本作为一套题让机器从中学习到规律，这套题我们叫【训练集】；那么另外剩下的30条样本作为考试用的试题来检验一下机器学习的成果如何，这套题我们叫【测试集】。

OK，我们用专业术语来形容一下上面的实战迷你版本：老师拿到一个数据集，里面有100条样本，每个样本有3个特征，老师要做的是用这三个特征来预测标签值为多少。那么老师在建模的时候就需要把数据集分成训练集和测试集，用训练集训练完成后使用测试集来检验模型效果。

相信这点东西难不倒你，那就让我们move on到更加细节一些的知识吧！

#### 2.2 机器学习分类

毕竟，没有人可以从事世间所有的行业，术业有专攻嘛。机器学习的模型也是一样的，你要知道哪个行业适合你，就和你要知道哪种数据集适合哪种模型一样。下面我们来看看其中的两种分类方式：

**根据标签值**

说到数值，我们就会想到连续的和不连续的。比如我们之前的例子中的房价就是个【连续值】，因为他有无数种可能的取值。可是如果我们预测的是标签是“是否有升值空间”，那无非就两种结果，会升值和不会升值，我们管这种数值叫【离散值】。

所以在机器学习中，我们也要根据我们的标签是离散的还是连续的来选择对应的学习方法。如果是离散的，我们称这种学习任务为【分类】，顾名思义，就那几种类别，分一下就好；如果是连续的，例如我们今天的项目，这种学习任务我们叫【回归】。

**根据有无标签**

有同学就要问了，还能没有标签？当然能！我们拿到的数据标签如果不是自动记录在册的（比如售房记录，一定会有售价），那就是我们人类自己一条一条人工标记的（比如说在人脸识别数据集中我们要自己标记哪里是眼睛哪里是嘴巴）。可是大数据时代每天的海量信息是无法有足够的人力来标记的。所以在工业界，如果公司没有经费请人标记标签，很有可能机器学习工程师拿到的数据是没有标签的，或者只有一部分有标签。
那么这种情况工程师就有三种选择：

1. 如果有标签：我们管这种学习叫做【监督学习】，也是准确率最高的一种学习方法。我们之前提到的分类、回归都是属于监督学习。
2. 如果没有标签：我们管这种学习叫做【无监督学习】，这种任务常常用来【聚类】，也就是说机器并不能学习到每条数据应该有的标签值，但是它可以学习到有哪些样本很相近，它们应该属于同一个未知标签。
3. 如果标签不全：我们管这种学习叫做【半监督学习】，是监督学习和无监督学习的结合。

我们画一个图来直观的看一下：

<p align="center">
  <img src="pics/4.png">
</p>

那么很容易就可以认识到我们今天要做的项目应该使用监督学习中的回归算法。说到这，同学就会好奇那什么算法是监督学习中的回归算法呢？

我们接下来就是要介绍一些针对以上情况的不同热门算法。

#### 2.3 常见模型

我们不妨在之前的树状图上添加一些基础模型：

<p align="center">
  <img src="pics/5.png">
</p>

在后面的实战中我们的数据集是有标签的，并且标签为房价也就是连续值，所以我们不妨选择线性回归作为我们的学习器。在这里我们不展开这些模型内在的逻辑，毕竟不是三言两语能讲解清楚的，同学可以在课后百度一下，老师保证这些模型初学者都是可以自己理解的。

最后老师要提醒一句，同学在选择模型的时候千万不要贪图使用“最新”的模型，我们要针对自己手里的数据找最适合的模型。有时候“越复杂”的模型看起来很酷炫但是未必可以在我们的数据集上学习出最好的效果。所以在工业界这些基础模型反而是使用最多的。

从下一章节开始我们就要进入实战细节了，老师会先从宏观上告诉同学建模都需要做些什么，再深入线性回归这个模型来一探内在逻辑。



## 闯关练习

1. 下列描述正确的是：

A. 测试集是训练集的一部分
B. 无监督学习是没有标签的
C. 分类可以用到无监督学习中

答案：B，测试集是数据集的一部分，和训练集通常互斥。在无监督学习中的分类叫做聚类，而不叫做分类，分类特指的是监督学习的情况下。





## 第三关 不积跬步无以至千里

#### 3.1 建模步骤

很多同学学习机器学习都是一心扑在研究各种模型上，这其实有点本末倒置了。我们在做一个机器学习项目时往往80%的时间都是花在数据集上，最后的20%时间才是去建模。所以很大程度上如果前面的80%没有做好，最后建模会发现自己的模型怎么都没有别人的模型效果好，但其实我们差的不是模型的选择上，而是前置步骤做的不如别人。

那么前面的80%都需要做些什么呢？

**第一步：理解业务场景**

我们要知道我们的目的是什么。现在有两个场景：

第一个场景：我们要为银行预测借款人会不会逾期还款，如果会逾期还款银行也许就不会再借贷给这个人了。在这个场景中我们需要做的是尽可能的提高准确率，这个准确率达到95%以上就已经很好了。

第二个场景：我们要为医院预测病人是否得了恶性肿瘤，但是在这种情况下准确率往往就不再是我们的首要标准了。如果我们的模型预测出病人没有得病但是他确确实实是生病了，哪怕模型准确率是99%，但是因为那1%的可能病人的最佳治疗时间就会失去，这个风险是我们不想看到的。所以在这种场景下我们的目的就不再是提高准确率。

**第二步：获取数据**

当然啦，我们的项目中数据是已经给到同学的，但是工业中这些数据是需要工程师自己去搜集的。从哪里搜集？都要搜集哪些数据？如何去搜集？等等都是摆在工程师眼前的问题。

**第三步：数据预处理**

也可以叫做【特征工程】，主要是为了从我们第二步搜集的数据中提取、筛选、清洗出可以发挥最大效力的数据。这一步是机器学习工程师最最最重要的技能，也是会决定我们模型能力上线的一步。

在这一步中，我们会剔除掉我们认为对结果影响很小的特征；我们会把数据集中空缺的地方补全；我们会把一些特征转化成模型可以学习的形式等等。因为这一步过于重要，老师会在下一小节中详细介绍一下数据预处理的一些常规操作。

**第四步：训练模型**

挑选几个模型进行学习，现在有很多的机器学习库可以帮助我们“一键”开始训练模型。

**第五步：模型评估**

训练完模型之后，我们就可以使用测试集来看看模型的效果。如果效果欠佳，我们就要分析一下是哪里出现的问题呢？这时候我们也许就要返回第二步或第三步，直到我们的模型效果达到了我们的要求。评估方式有很多，例如对比准确率。

#### 3.2 数据预处理

**缺失值**

我们的数据有时是从互联网上爬下来的，有时是我们的用户提交的等等，这些数据一定存在一个共同的问题，就是数据不完整。我们管在数据集中空缺的数据叫做【缺失值】。如果某个特征的缺失值超过了我们的阈值（比如50%）我们可以考虑删除掉整个特征列，如果没有超过我们可以利用一下统计的方法来填充他们，比如取所有有值的平均数；还可以利用另一个模型来对缺失值进行预测等等。

**异常值**

也可以叫【离群点】，【异常值】是在数据集中存在的“不和谐”或者“不合逻辑”的值。“不和谐”的数据例如一套独栋别墅有20层楼，这种数据极其的不普遍而且很有可能是录入的时候造成错误多输入了一个0。“不合逻辑”的数据例如一套独栋别墅有0层甚至出现了负数。对于异常值的处理，我们可以借鉴缺失值的处理方法。

**特征选择**

我们想要考出好成绩，就要在有限时间内多复习和考试内容相关的东西。如果把我们手里有的东西一股脑儿的塞给模型，它会崩溃掉的，它将不知道把精力放在哪个特征上面才能提高成绩。我们可以通过特征相关性的计算来删除掉那些和标签取值不太相关的特征。

**数据变换**

我们知道机器只认得数字，所以如果数据集中有字符串的数据类型，我们可以通过【独热编码】来处理一下。独热编码是把特征中每一个种类分别算作一个新的特征列，例如下图中如果第一个样例的颜色是红色，那么就在红色一列标记1，其余为0，依此类推。

<p align="center">
  <img src="pics/6.png">
</p>

还有一些数据虽然本身是数字，但是差别太大。比如抽样了100个人的月收入，有的人800块钱，有的人可能800万。这么大的差异我们可以通过一些公式【归一化】到[0，1]的范围内。




## 闯关练习

1. 在房价预测的案例中，下列不属于异常值的是：

A. 房价标签低于正常分布
B. 最高的房价
C. 邮编号码不在地区范围内

答案：B，我们所说的异常或者说离群，这个“群”指的是该特征的分布情况，所以最高/最低的房价只要是相同分布就不是异常值。


2. 下列建模步骤描述错误的是：

A. 数据预处理是为了更好的学习特征分布
B. 训练模型是最重要的环节
C. 理解业务场景可以让我们明确优化目标

答案：B，数据的处理是最重要最困难最能影响到模型效果的环节，相同学习器之间很难将结果拉开差距。



## 第四关 知己知彼

#### 4.2 线性回归

这一关我们要介绍实战使用的模型【线性回归】，别看它简单，很多模型都是以它为基础的，比如逻辑回归和神经网络。那么什么叫线性？顾名思义就是我们要学习出一条直线y=ax+b，我们希望这条直线可以拟合数据集。例如

<p align="center">
  <img src="pics/7.png">
</p>

这是在二维平面上的线性回归，也就是有两个轴，如果我们用x表示卧室数目，y表示房价，那么我们学出来的直线就会类似上图，x越大，y也就越大。

在数据集中我们显然已经有了x和y的数值，因为我们的任务是监督学习，那如果要求出这条直线就求解y=ax+b中的a和b的值即可。

那这是只有一个x的情况，可是我们有很多的特征x，那也没关系，再往上加就好，比如说我们要拟合一下爱迪生的名言：

<p align="center">
  <img src="pics/9.png">
</p>

此时我们用x1表示汗水，x2表示天赋，b表示性格，预测你成功的指数是多少。在这里我们把a换成了w，在线性回归中w表示【权重 weight】，指的是对应的特征的重要程度。在上面的模型中表示汗水比天赋对成功的影响要大得多。

我们还可以随着数据集的特征数量增加而增加更多的w和x。那么类似的我们仍然可以写出来一个表达式：

<p align="center">
  <img src="pics/8.png">
</p>

这个就是线性回归的内在逻辑，模型会通过学习自己更新每个w的值和b，来寻找每个特征对标签的影响力大小。

那么模型是如何自己学习的呢？

#### 4.1 代价函数

我们在学习中要想提高成绩，必须要知道的就是自己每次做题错在哪最终得了多少分，然后通过100分-你的得分算出你还有多少需要提高。

在线性回归中，我们一样可以使用这个逻辑来计算模型还有多少需要提高。假设第一个样本预测的房价为100万，但是实际标签为110万；第二个样本通过模型预测出来是80万，但实际是70万。我们就可以用（110万-100万）+ （70万-80万）来当作模型的误差。

但是细心的同学会发现（110万-100万）+ （70万-80万） = 0万。。可是我们预测值的差距每个都有10万应该是20万呀。没错，为了避免这个问题，我们可以给他们加一个平方，这样（70万-80万）的平方就不会是负数了。那有的同学又要问了，加了平方不就改变了原本的误差了么？没错，确实改变了，但是我们不在乎。因为我们的要求只是这个误差值越小越好，无所谓具体取值，只要保证不改变原本的分布就好。

那么放在我们的数据集中我们就有了这样一个计算总体误差的函数：

<p align="center">
  <img src="pics/10.png">
</p>


带个小帽子的y是我们每个样本的预测值，减去实际标签的值，再平方。我们把数据集中所有m个样本都计算一遍，相加起来求个平均数。
这个函数叫做【均方误差 MSE】，几乎可以应用到所有回归模型中。另外，这种用于计算模型误差的函数叫做【代价函数】，顾名思义就是每次告诉我们模型还有多少需要提高，所以我们希望最小化这个代价。

通过这个函数，我们就可以告诉模型你下一步应该怎么样去改变权重w和b的值，以便更好的拟合出一条直线。至于怎么去更新权重w和b的取值，需要用到的方法叫【最小二乘法】，很简单，同学学有余力的话可以百度一下。

到目前为止，我们已经掌握了所有实战的前置知识，那还等什么，让我们开始写代码吧！


## 闯关练习

1. 下列关于线性回归描述错误的是：

A. 学习到的w值是每个特征和标签的相关性大小
B. 代价函数可以使用MSE
C. 也可以拟合曲线

答案：C，线性回归因为x是一次方，所以无论如何改变w和b，永远只能拟合出一条直线。



## 第五关 绝知此事要躬行

#### 5.1 实战之业务场景

我们严格按照之前讲的建模顺序来！首先我们要明白我们的业务场景：我们正在为一家房产公司打工，老板想通过已经完成交易的数据来对未来房子的售卖有一个更加专业的估价。OK，那我们就知道了我们要优化的是准确率。
数据老板可以给到我们，不需要我们人工搜集信息了。拿到数据之后，我们首先要了解一下数据里都有什么？我们使用心爱的编辑器创建一个新的python文件，然后导入一些应该会用到的包：

```
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr # 求皮尔逊相关系数
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
```

接下来就是导入我们的数据，看一看长什么样子：

```
data = pd.read_csv("kc_house_data.csv")
data.head()
```

<p align="center">
  <img src="pics/12.png">
</p>

我们还可以直接info来看一看：

```
data.info()
```

<p align="center">
  <img src="pics/13.png">
</p>

除了0、1、2列的编号日期和价格之外，其余的都是关于这个房屋的配置，比如几个卧室几个厕所几层楼修建年份视野如何生活面积地下室面积等；还有一些这个的位置信息比如lat、long、zipcode就是经纬度和邮编；还有一些房屋的状态信息比如condition是和新的一样还是略旧。

粗略的了解了一下我们都有的是什么之后就可以开始对这些数据进行预处理了。

#### 5.2 实战之数据预处理

我们依然按照之前讲的预处理要点来处理他们

**缺失值的处理**

我们先看一下有没有缺失值：

```
data.isnull().any()
```

<p align="center">
  <img src="pics/14.png">
</p>

显而易见，我们的建造年份有缺失值。除了可以通过isnull函数还可以通过之前的info函数来看，从之前的info信息看建造年份的信息缺了3个。数量不多，我们可以大概估算一下年份，比如用其余的年份求一个平均数：

```
avg = data["yr_built"].mean() # 求出平均数
data = data.fillna(value=int(avg)) # 填充平均数到空缺值上
```

OK，这样我们的数据就更加完整了。

**异常值的处理**

当然这个先后顺序没有明确要求哈，如果同学想先处理异常值再填充缺失值也许是会更精准的。
想发现异常值不是那么简单的事情，有很多专业的算法在做这个事情，我们不想涉及那么深，对于初学者我们可以使用describe函数来观察一下特征的分布情况：

```
data.describe()
```

<p align="center">
  <img src="pics/15.png">
</p>

它帮我们统计了每一列的个数、平均数、标准差、最大最小值和统计上的百分位数。我们粗略观察到bedroom和bathroom的最小值是0。还有没有卧室和厕所的房子吗？也许是住宅商用？不管了，我们看一下当卧室和厕所包含0的这些房子有多少个：

```
len(data[data.bedrooms == 0]）
len(data[data.bathrooms == 0]）

output：
13
10
```

一共23个，不多，可以考虑直接删除掉避免它们是因为人工录入错误。

```
data = data[data.bedrooms != 0]
data = data[data.bathrooms != 0]
```

其余的信息大体上看没有很突出的问题，我们就姑且放过他们。

**特征选择的处理**

特征选择无非就是找出那些对结果影响大的特征，那么我们可以利用统计上的皮尔逊相关性计算方法来看看每一列特征和价格之间的相关性：

```
features = data.iloc[:,3:].columns.tolist() # 拿出所有想要计算相关性的特征
label = data.iloc[:,2].name # 拿出标签
correlations = {} # 创建一个特征与相关系数的字典
for f in features:
    data_temp = data[[f,label]]
    x1 = data_temp[f].values
    x2 = data_temp[label].values
    key = f + ' 与 ' + label
    correlations[key] = pearsonr(x1,x2)[0] # 调包计算相关性存入字典中
data_correlations = pd.DataFrame(correlations, index=['相关系数']).T
data_correlations.loc[data_correlations['相关系数'].abs().sort_values(ascending=False).index]
```

<p align="center">
  <img src="pics/16.png">
</p>

可见生活面积还是和房价最相关的特征。我们可以使用相关系数大于0.1的特征，把编号日期价格和最后几个小于0.1的特征都删掉：

```
X = data.drop(columns=['id','date','price','sqft_lot', 'sqft_lot15', 'yr_built','zipcode','condition','long'])
Y = data['price']
```

现在所有需要的特征都在X里，我们要预测的标签在Y里。

除了通过统计学的方法进行特征选择，我们通常还会搭配可视化，例如我们可视化一下生活面积和房价的关系：

```
plt.scatter(X['sqft_living'], Y)
plt.show()
```

<p align="center">
  <img src="pics/17.png">
</p>

可见正相关的很啊，而且还可以通过可视化看出一下异常值/离群点，比如红框框起来的点。
由于特征太多了我们就不一一可视化了，同学可以课后模仿生活面积观察一下其余的特征。

**数据变换的处理**

我们之前也发现了我们特征里并没有字符串信息，就不需要做独热编码了，但是我们的数据里因为数值差异比较大，有的几千有的几万有的则是个位数，我们可以归一化一下他们，使他们保持在[0，1]之间：

```
normalized_X=(X-X.min())/(X.max()-X.min())
```

<p align="center">
  <img src="pics/18.png">
</p>

好，这样我们就把所有的值都归一化到0-1之间了。

数据预处理我们就做到这里，接下来就是训练模型了。

#### 5.3 实战之训练模型

训练模型我们需要训练集和测试集，我们首先把他们分开一下：

```
# xtrain：训练集的特征
# xtest：测试集的特征
# ytrain：训练集的标签
# ytest：测试集的标签
xtrain, xtest, ytrain, ytest = train_test_split(normalized_X, Y, test_size=1/4, random_state=0)
```

然后调用线性回归并且让它拟合训练集：

```
model = LinearRegression()
model.fit(xtrain, ytrain)
```

#### 5.4 实战之模型评估

训练完成后我们在测试集上看一看预测的效果

```
model.score(xtest,ytest)

output:
0.6661383416105844
```

但是因为我们是回归模型，所以最主要的指标是之前说的MSE，我们要最小化的是它，那我们看看训练完它是多少：

```
from sklearn.metrics import mean_squared_error
ypred = model.predict(xtest)
mean_squared_error(ytest, ypred)

output:
39745596032.64976
```

还记得我们当时讲线性回归的时候写的公式么，每个特征就是一个x，跟随它的有一个w，最后整体再加一个b。
我们可以查看一下当前模型的w和b是多少：

```
pd.DataFrame(list(zip(X.columns, np.transpose(model.coef_)))) # 查看所有w
model.intercept_ # 查看b
```

<p align="center">
  <img src="pics/19.png">
</p>

为了形成效果对比，老师用我们提到的回归另一种方法【多项式回归】再预测一下（同学无需理解，看效果就好）：

```
from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2, include_bias=False) # 创建多项式回归的特征构造器
xtrain_poly = poly_features.fit_transform(xtrain) # 用构造器创建新的训练集特征

model.fit(xtrain_poly, ytrain) # 拟合模型

xtest_poly = poly_features.fit_transform(xtest) # 用构造器创建新的测试集特征
print(model.score(xtest_poly,ytest)) # 打印出得分

ypred = model.predict(xtest_poly) # 通过测试集预测标签
print(mean_squared_error(ytest, ypred)) # 求预测标签和真实标签之间的MSE

output:
0.7431233215742521
30580680453.608913
```

是不是比我们的线性回归的MSE更加低了一些？多项式回归拟合的就不再是直线了，它可以拟合曲线，如下图。所以效果会比线性回归好一些。

<p align="center">
  <img src="pics/20.png">
</p>


## 课后总结

<p align="center">
  <img src="pics/11.png">
</p>

本节课我们的内容还是满多的，我们先从机器学习的背景和生活中的实例开始讲起，然后宏观上了解了一下机器学习的分类和建模的步骤，再然后我们详细理解了建模前最最最重要的一步【数据预处理】。我们挑选了线性回归作为我们的学习器，所以简单讲了一下线性回归的内在逻辑。最后我们终于完成了一个可以预测房价的模型。

按照为师的惯例，留个小作业。我们列举了一些常用的模型，与其我们纠结选择用哪个，那我们可不可以同时使用多个？俗话说的好，三个臭皮匠胜于诸葛亮，这种多模型的学习方法叫做【集成学习】，是无论竞赛还是工业上经常使用到的技巧，内容不难，也是初学者可以自学的内容。（老师的代码中最后有彩蛋，看看是不是比多项式回归还要厉害？）

通过本节课的学习，同学已经算是入门了机器学习，如果还想了解更多的模型和原理，可以关注老师未来的课程，或者报名参加我们的视频课程。
